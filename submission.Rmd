---
title: "Reproducibility of published meta-analyses on clinical psychological
  interventions"
shorttitle: "Meta-analyses reproducibility"
leftheader: "Lopez-Nicolas et al."

author:
 - name: "Rubén López-Nicolás"
   affiliation: "1"
   corresponding: yes
   address: "Facultad de Psicología, Campus de Espinardo, Universidad de Murcia, edificio n° 31, 30100, Murcia, Spain"
   email: "rlopez@um.es"
 - name: "Daniel Lakens"
   affiliation: "2"
 - name: "Jose A. López-López"
   affiliation: "1"
 - name: "Maria Rubio-Aparicio"
   affiliation: "3"
 - name: "Alejandro Sandoval-Lentisco"
   affiliation: "1"
 - name: "Carmen López-Ibáñez"
   affiliation: "1"
 - name: "Desirée Blázquez-Rincón"
   affiliation: "1"
 - name: "Julio Sánchez-Meca"
   affiliation: "1"

affiliation: 
 - id: "1"
   institution: "University of Murcia, Spain"
 - id: "2"
   institution: "Eindhoven University of Technology, The Netherlands"
 - id: "3"
   institution: "University of Alicante, Spain"
    

abstract: |
  Meta-analysis is one of the most useful and powerful research approaches, the relevance of which relies on its credibility. However, in recent years different concerns about the credibility of psychological research have emerged. Reproducibility of scientific results could be considered as the minimal threshold of it.  In this study, our purpose was to assess the  reproducibility of a sample of published meta-analyses. From a random sample of 100 papers containing at least one meta-analysis on the effectiveness of interventions in clinical psychology, 217 meta-analyses were selected. We first tried to retrieve the original data by recovering a data file, recoding the data from document files (pdf, doc), or requesting it from original authors. Second, through a multi-stage workflow, we tried to reproduce the main results of each meta-analysis using these data. The original data were retrieved for 146 \textcolor{blue}{(67\%, 146/217)} meta-analyses. Of these, 52 showed a discrepancy larger than 5% in the main results in the first stage. \textcolor{blue}{For 10 meta-analyses this discrepancy was solved after fixing a coding error and for 15 of them it was solved with minor adjustment or considered approximately reproduced in a qualitative assessment.} In the remaining 27 meta-analyses\textcolor{blue}{(18\%, 27/146)}, different issues were identified in an in-depth review of the papers, such as reporting inconsistencies, lack of data, or transcription errors. \textcolor{blue}{Nevertheless, the numerical discrepancies produced by these issues were mostly minor, with little or no impact on the conclusions.} Current practices of data sharing in meta-analyses hamper the reusability of meta-analytic data. The implementation of new tools would help to avoid certain errors in the meta-analysis reporting process.

keywords: "meta-analysis, reproducibility, data sharing, data reusability, research synthesis"

bibliography: ["meta-analyses_reproducibility.bib"]

figsintext        : yes
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : no
mask              : no
csl               : "apa.csl"
nocite            : |
                    @koffel016, @maggio2011, @nguyen2022, @page2018, @page2016, @tendal2011, @wayant2019

class             : "man"
output            : papaja::apa6_pdf
header-includes:
   - \usepackage{caption}
   - \captionsetup[table]{textfont={it}, labelfont={bf}, singlelinecheck=false, labelsep=newline}
   - \captionsetup[figure]{textfont={footnotesize, it}, labelfont={bf}, singlelinecheck=false, labelsep=newline}
---

Meta-analysis is widely considered as an important approach to evaluate a body of work. Given the ongoing growth in the number of scientific publications [@bornmann2021], evidence synthesis approaches—such as meta-analysis—are becoming increasingly relevant for a cumulative science. This relevance rests on the credibility of meta-analytic results, which can be threatened by a lack of rigorous methodology or poor-quality reporting [@gurevitch2018]. Given the importance of meta-analyses for evidence-based practice, these threats to their credibility need to be closely monitored.

In recent years different concerns on the credibility of  empirical claims have emerged. Several projects have systematically attempted to assess the replicability and reproducibility of published scientific results (e.g., @artner2020; @errington2021a; @opensciencecollaboration2015). Those initiatives showed many failures to replicate or reproduce the published results. In this context, the empirical assessment of the credibility of published results has become a major task for the scientific community.

There are different approaches to the empirical assessment of scientific credibility. Reproducibility refers to the attempt to obtain the same results as in the original publication, using the same data and the same procedure. Robustness refers to the assessment of the sensitivity of the originally published results and conclusions to variations in the original procedure using the same data. Replicability is a core principle of the scientific method and refers to the fact that the same scientific evidence should be observed when independent researchers try to answer the same research question from the same approach at different moments using different data. In other words, obtaining the same results, using different data and answering the same question [@nationalacademiesofsciencesengineeringandmedicine2019; @nosek2022]. In this project, we focus on the reproducibility of meta-analyses.

The reproducibility of published scientific results could be considered as the minimal threshold of scientific credibility [@hardwicke2021]. Different approaches can be adopted for the empirical assessment of reproducibility. For example @nosek2022 make the distinction between process reproducibility and outcome reproducibility. Following this framework, a process reproducibility assessment could be carried out by reviewing the availability of the materials, data, or precise details of the analytical strategy in the report that are required to proceed with the reproduction attempt. An outcome reproducibility assessment can be carried out when the required elements are retrievable by actually reproducing the analyses. It is worth noting that the difficulty of performing an outcome reproducibility assessment depends on which analytical information is available. The availability of the original analysis code (i.e., the original computational instructions in a programming language) facilitates reproducibility analysis by enabling simply re-running the code on the data. Regrettably, the analysis code is currently seldom available [@hardwicke2020; @hardwicke2022; @lopez-nicolas2021]. When only a verbal summary of the performed analyses is available in the research report (which is the most common scenario in practice), the original analysis needs to be reconstructed. The challenges and implications of failed reproductions in both cases may be of a different nature. 

Several reproducibility analyses of meta-analyses have been performed in recent years. For example, some process reproducibility assessments have shown an important lack of data availability in machine-readable formats, and an almost complete absence of analysis script code availability [@lopez-nicolas2021; @polanin2020]. Furthermore, some outcome reproducibility assessments have shown a considerable number of failures when trying to reproduce the primary effect sizes of some published meta-analyses by recollecting primary data from primary studies [@gotzsche2007; @maassen2020; @tendal2009], possibly due to lack of details on how primary effect sizes were selected and computed. In these outcome reproducibility studies, the main task entails reconstructing the original data by retrieving them from the source, namely the included primary studies. Thus, their assessment focus is on this stage of the analysis pipeline of a meta-analysis, which usually involves decisions on how to select the primary outcomes and how to deal with possible dependency, and the computation of (standardized) effect sizes. Figure 1 displays a summary of the basic meta-analysis pipeline through a flowchart, outlining the different stages and listing previous work that has explored different facets of reproducibility of these, as well as a summary of the required elements to be able to reproduce each stage. \textcolor{blue}{In this project we focus on the last stage, related to the statistical analysis and quantitative results of the synthesis.}

\begin{figure}
\begin{center}
\includegraphics[width=\textwidth, height=\textheight]{results/Figure 1.png}

\caption{Flowchart displaying the basic pipeline of a meta-analysis. Each of the stages may be subject to reproducibility evaluation. On the left, known studies that have evaluated some facet of the reproducibility of each stage are listed. On the right, the various elements that must be available to reproduce each stage are enumerated.
}

\label{fig:figs1}
\end{center}
\end{figure}


Reproducibility analysis \textcolor{blue}{of reported quantitative results} typically uses the original data available from the original authors [e.g., @artner2020; @hardwicke2021; @hardwicke2018]. This puts the focus of the assessment at factors such as the reusability of the available data, challenges for the reconstruction of the original analysis scheme, reporting errors, etc. Although data availability seems to have improved in the last years [@hardwicke2018; @tedersoo2021; @wallach2018], systematic reviews and meta-analyses appear to be a special case. Typically, the data collected for a meta-analysis is study-level summary data extracted from published primary studies which is commonly reported in the paper through tables or forest plots. This may lead to the idea that common data sharing practices do not apply to meta-analysis. For example, @page2022 analysed the content of data availability statements from a set of meta-analyses published in 2020. Only 31% included a data availability statement and only 13% of these included a link to access the data openly, with 23% stating that all relevant data are available in the paper itself, 10% stating that data sharing is not applicable as no datasets were generated, 8% stating that data sharing is not applicable as the data is drawn from already published literature, and 42% stating that data were available upon request. It is surprising that, even just considering meta-analyses that included a data availability statement, the authors of these meta-analysis  assume that such practices do not apply to meta-analyses, or that the data in the article itself is sufficient.

## Purpose 
Previous research has revealed that there is room for improvement at different stages of the meta-analytic process pipeline. In this study our purpose is twofold. First, we broadened previous process reproducibility assessments by considering data availability on request and by contacting original authors to request required information to reproduce the meta-analysis. Second, we verified the outcome reproducibility of the meta-analyses that were process-reproducible using the available data. Where previous work focused on the reproducibility of primary effect sizes, we explored meta-analysis outcome reproducibility using the primary effects sizes already coded by the original authors. 

```{r importing_data_and_descriptives, include=FALSE}
 
library(here)
source(here::here("analysis", "01_importing_data_and_descriptives.R"))

```

# Method
## Preregistration
The pre-data analysis protocol (https://doi.org/10.17605/OSF.IO/79J2T) was pre-registered on 19 October 2021. Any deviation from this protocol is explicitly acknowledged.

## Data, materials, and online resources

Data and analysis script code are openly available at: https://osf.io/6cmzh/

## Identification and selection of articles and meta-analyses 
In previous research we identified a pool of 664 meta-analytic reports on clinical psychological interventions published between 2000 and 2020 through a systematic electronic search [@lopez-nicolas2021]. Of this pool, 100 were randomly selected using a random number generator between 1 and the total number of meta-analyses identified. The full search strategies and a summary of the screening process are available at: https://osf.io/z5vrn/, and the workflow of the random selection process is available at: https://osf.io/cp293/. \textcolor{blue}{This sample size was based on our judgement of an acceptable trade-off between informativeness and feasibility.}
From these 100 articles, each independent \textcolor{blue} {pairwise meta-analytic model of aggregate data fitted} on at least 10 primary studies was selected. In case no meta-analysis reported in a paper had at least 10 studies, the meta-analysis with the highest number of primary studies was selected, which was the case for `r n_mas_less_10` of the articles included in this report. \textcolor{blue}{This criterion was established to focus on the main meta-analyses of each paper, based on the assumption that the search strategies would be designed to maximise the number of primary studies included that were related to the main aims of the paper.}

Our unit of analysis was each independent meta-analysis selected under these criteria. A total of `r n_mas_total` independent meta-analyses were selected.

## Retrieval of primary data
In order to be able to reproduce meta-analyses of \textcolor{blue}{aggregate data, primary-level}^[By primary-level data we mean aggregate data from included primary studies.] effects sizes and their associated standard errors are required. These are generally computed from statistics retrieved from the primary studies such as means, standard deviations or sample sizes. We attempted to retrieve the least processed data possible. First, we sought for the statistics \textcolor{blue}{used to compute primary effect sizes (e.g., means, sd)}; second, we sought for the primary effects sizes \textcolor{blue}{already computed} and their standard errors (or, alternatively, the sampling variances): finally, we sought for the primary effects sizes and their confidence limits, from which the standard errors were approximated as follows: 

$$se_i = (\frac{UB_i - LB_i}{2z_{\alpha/2}})$$

with $se_i$ being the standard error of the ith effect size, $UB_i$ and $LB_i$ the upper and lower confidence limits of confidence interval for the ith effect size, and $z_{\alpha/2}$ the $(1 - {\alpha/2})\%$ percentile of the standard normal distribution (usually, $z_{\alpha/2} = 1.96$ assuming a two-sided 95% confidence interval).

\textcolor{blue}{On the other hand, efforts were also made to retrieve the most reusable data possible.} First, we searched for machine-readable data files through links leading to third-party repositories or in supplementary material hosted by the journal. Second, we looked for available data through tables or forest plots in the meta-analytic report itself, or in supplementary material. In these cases, the primary data had to be manually re-coded to reuse it. Finally, if the primary data of a meta-analysis were not directly available after the previous steps, we attempted to obtain the data through a request to the corresponding author identified in the associated paper. We sent an initial request in June 2021 and, if there was no reply, a subsequent reminder in October 2021. This reminder was sent to a more recent alternative email address if we were able to find one. If we were unable to obtain the data through the email request, the associated meta-analysis was labelled as not process reproducible.

## Reconstructing the original analytical scheme
To proceed with reproducibility attempts of the meta-analyses that were labelled as process reproducible, we first looked for the availability of the original analysis script. When it was available, reproducibility was checked by rerunning the original script on the associated primary data. When it was not available, we tried to reconstruct the original analytical scheme using the technical details reported in the paper. Specifically, we collected information on: (a) the meta-analytic model originally assumed; (b) the weighting scheme; (c) the between-studies variance estimator; (d) the method used to compute the confidence interval; and (e) the software used to perform the meta-analysis. If any of these details about the analytical methods were not reported, but the software used was mentioned, we inferred the first four pieces of information from the default settings of the software used. If the software used was not reported, we inferred this information from the default settings of the most used software in the sample, which was *Comprehensive Meta-Analysis*. \textcolor{blue}{We designed this procedure to reconstruct the original analytical scheme when the original analysis script was not available instead of trying to request it from the original authors due to: (a) not necessarily all authors of included meta-analyses will actually have an analysis script to share, because many might have used point and click software, and (b) we expected analysis script availability to be very low, and requesting it would have meant sending request for virtually every paper included in our re-analysis.}

Additional information about the meta-analysis was collected that is not reported in this manuscript. The full list of variables collected is available in the Protocol (https://osf.io/tq4uf/) and a Codebook describing these variables is available at: https://osf.io/ym78s/. 

## Data collection procedure 
\textcolor{blue}{Data collection procedure was carried out by five of the authors.} At a first pilot stage, a random sample of five articles of the total pool was independently coded by the five members and, subsequently, in a series of meetings, disagreements between the coders were resolved by consensus. Next, the initial pool of 100 included articles was split among four coders, 25 articles each. A random sample of 25 articles of the total pool was assigned to the fifth member to carry out independent double-coding, with the goal to examine the reliability of the data collection process. Disagreements were resolved by consensus and by double-checking the original materials. \textcolor{blue}{Details about inter-coder agreement are reported in the Supplementary file.}

## Reproducibility outcomes
Each meta-analysis was labelled using the following \textcolor{blue}{two-level}^[This hierarchy is a minor deviation from the pre-registered protocol. It is essentially the same and the results are identical. It was introduced to improve clarity.] reproducibility success scheme. \textcolor{blue}{First, each meta-analysis was labelled as: (a) process-reproducible; and (b) not process-reproducible. In our study, not process-reproducible refers to situations where we were unable to access the primary data neither through direct extraction nor upon request}^[Process reproducibility, as described above, could imply a different situation if more conditions need to be met to proceed with the reproduction attempt. In our study, this is equivalent to data availability due to our design and the stage of the meta-analysis pipeline we focused on.]. \textcolor{blue}{Second, those labelled as process-reproducible} were labelled as: (a) reproducible; (b) numerical error; and (c) decision error.  Similar to previous studies (Artner et al., 2020; Hardwicke et al., 2018, 2021) an index of numerical error was computed (see Protocol https://osf.io/tq4uf/). This index expressed the difference between reproduced and original values as a percentage. To avoid labelling minor numerical discrepancies related to numerical rounding as reproducibility problems, a 5% discrepancy threshold was set. Thus, a meta-analysis was labelled as 'numerical error' if it showed a discrepancy larger than 5%.^[A sensitivity analysis using other possible criteria is reported in the supplementary file.] 
Finally, the label 'decision error' refers to situations where the $p_{reported}$ fell on the opposite side of the .05 boundary in relation to the $p_{reproduced}$. 

We focus on reproducibility of summary effects, their confidence bounds and the result of the null hypothesis significance test. Secondarily, we also assessed reproducibility of other synthesis methods such as heterogeneity statistics.

## Reproducibility checks workflow 
Reproducibility checks were carried out at different stages. First, through reported analytic details or script code. When the analysis script code was available, computational reproducibility was checked by rerunning the script with the available primary data. In most cases, the analysis script code was not available. Thus, in these cases we coded the analytic details as explained above to fit equivalent meta-analytic models as a function of these details using the available primary data. This analysis scheme was programmed in the R environment [@r2022] using the *metafor* package [@viechtbauer2010].

Second, given that the manual recoding process is an error-prone task, some mistakes can appear. Thus, those meta-analyses labelled as numerical error and/or decision error in the previous stage were re-assessed by a different member of the team. In cases where an error was found in the originally coded results, analytic methods and/or primary data, the meta-analyses were once reproduced again and re-labelled according to the updated results. Additionally, a qualitative assessment of the meta-analyses still labelled as numerical error and/or decision error was also carried out. The same reviewers who checked for errors produced individual reports on the possible source of the discrepancy and its reproducibility was judged qualitatively by four of the other authors. This stage was a deviation from the pre-registered protocol, and made it possible to identify situations with obvious explanations, such as rounding issues, inverted signs, etc.

Additionally, for meta-analyses that remained labelled as non-reproducible, an email was sent to the corresponding author of the associated paper explaining our aims, our approach, and our results regarding his/her meta-analysis and requesting additional information that could explain the mismatch between the original reported results and the reproduced results. We tried to solve the reproducibility issues within a month after the request and we updated the label accordingly.

\textcolor{blue}{Finally, the association between publication year and the possibility of retrieving the data in one of the ways conducted in this project were explored by fitting binary logistic regression models with publication year as predictor and process-reproducibility as dependent variable. We quantified the strength of the association by calculating odds ratios and 95\% confidence intervals based on the profile likelihood. These exploratory analyses were  not pre-registered. Details and results are reported in Supplementary file.}

# Results 

From the 100 included papers, `r n_mas_total` independent meta-analyses were selected following the criteria explained above. These meta-analyses included `r round(k_mean, 2)` primary studies on average (`r sprintf("sd = %0.2f; median = %0.0f; interquartile range = %0.0f-%0.0f; range = %0.0f-%0.0f)", k_sd, k_median, k_q1, k_q3, range[1], range[2])`), \textcolor{blue}{and were cited `r round(cc_mean, 2)` times on average (`r sprintf("sd = %0.2f; median = %0.0f; interquartile range = %0.0f-%0.0f; range = %0.0f-%0.0f)", cc_sd, cc_median, cc_q1, cc_q3, cc_range[1], cc_range[2])`}^[Citation counts were retrieved from CrossRef API using the *rcrossref* package [@rcrossref]. For two cases in which CrossRef did not return data, citation counts were consulted in Google Scholar. Both queries were done on 20/03/2023.]. 
Figure 2 displays the distribution of number of primary studies among the meta-analyses included in our sample \textcolor{blue}{(panel A), the publication year distribution among the papers included in our sample (panel B) as well as the citation count distribution of those papers (panel C).} Original results and characteristics of these meta-analyses are available at: https://osf.io/8jzbk

``` {r, fig.width = 12, fig.height=4, fig.cap="Distribution of (a) the number of primary studies included in each of the meta-analyses; (b) the publication year of the included papers; (c) citation count of the included papers. Vertical blue dotted lines represent the first quartile, median, and third quartile, respectively."}

figure2

```

## Process reproducibility 
Figure 3 summarizes the primary data retrieval results. Based on the availability of primary data, either retrieved directly from the paper or upon request, `r df_process_rep$n[2]` meta-analyses (`r sprintf("%0.0f%%", df_process_rep$perc[2]*100)`, see Fig. 3a) were labelled as process reproducible. Of these `r df_process_rep$n[2]` meta-analyses, in about half of the cases the primary data was retrieved from a forest plot in the paper itself and in about a third of the cases the primary data was retrieved from supplementary files (see Fig. 3b for further details). 
Although attempts were made to retrieve data for `r df_process_rep$n[1]+df_source_primary_data$n[1]` meta-analyses from `r sum(df_request$n)` different papers by emailing the corresponding authors, data was only retrieved for `r df_source_primary_data$n[1]` meta-analyses, from `r df_request$n[2]` different papers (`r sprintf("%0.0f%%, %0.0f/%0.0f", df_request$perc[2]*100, df_request$n[2], sum(df_request$n))`, see Fig. 3c). For the remaining `r df_process_rep$n[1]` from `r df_request$n[1] + df_request$n[3]` different papers, a reply providing some reasons not to share was received in `r sprintf("%0.0f%% (%0.0f/%0.0f,  see Fig 3c)", df_request$perc[3]*100, df_request$n[3], sum(df_request$n))`, whereas no reply was received for the remainder of the meta-analyses. Table 1 summarises the different reasons corresponding authors given when data was not provided upon request.

```{r}

kbl(
  df_reasons,
  format = "latex",
  booktabs = TRUE,
  col.names = c("Reason", "N"),
  align = c("l", "c"),
  caption = "Reasons given when data was not received upon request."
  ) %>% 
    column_spec(1, width = "16cm")
  
```

## Challenges faced retrieving primary data
In most cases, when the meta-analytic data was available, it was shared in document formats. Data \textcolor{blue}{were retrieved from} tables or forest plots in *pdf* or *docx* format—either in the document itself or in the supplementary materials—\textcolor{blue}{in} `r sprintf("%0.0f%% (%0.0f/%0.0f)", (df_source_primary_data$perc[3]+df_source_primary_data$perc[4]+df_source_primary_data$perc[6])*100, df_source_primary_data$n[3]+df_source_primary_data$n[4]+df_source_primary_data$n[6], sum(df_source_primary_data$n))` \textcolor{blue}{of the cases}. This required a manual recoding of the primary data to be able to reuse them. Furthermore, when data was reported through general tables (i.e. tables listing all the primary studies included with their characteristics), the meta-analysis associated with each data entry was not always obvious, leading to the time-consuming task of matching each data entry with each independent meta-analytic result reported in the paper. There were only `r df_source_primary_data$n[2]` meta-analyses (from `r words(sum(df_data_downloaded$Item))` different papers) of the `r df_process_rep$n[2]` meta-analyses labelled as process reproducible (`r sprintf("%0.0f%%", df_source_primary_data$perc[2]*100)`), where the task of retrieving the data required simply downloading the data in an machine-readable data file format.
On the other hand, as shown in Figure 3c, when the necessary data was not available, retrieving it upon request to the original authors led to a low response rate.

``` {r, fig.width = 10, fig.height=11, fig.cap="Percentage of (a) process-reproducible meta-analyses; (b) different types of sources of original data; (c) data request results."}

 figure3

```

## Outcome reproducibility 
The outcome reproducibility was checked in `r df_process_rep$n[2]` meta-analyses from `r nrow(df_nmas_wdata)` different papers. As mentioned above, in `r filter(df_script, script == 1)$n` of these meta-analyses (`r sprintf("%0.0f%%", filter(df_script, script == 1)$n/sum(df_script$n)*100)`), all from the same published article, the original script code was available. Therefore, in these five cases, outcome reproducibility was checked running the original analysis script on the original primary data. In the remaining cases, the original analytical framework was reconstructed as explained in the method section. 
Figure 4 summarises the results of the whole process of outcome reproducibility assessment. 

\begin{figure}
\begin{center}
\includegraphics[width=\textwidth]{results/Figure 4.png}

\caption{Results of the different stages carried out in the evaluation of the outcome reproducibility. 
}

\label{fig:fig4}
\end{center}
\end{figure}

```{r first_stage, include=FALSE}

rm(list=ls())
source(here::here("analysis", "02_first_stage.R"))

```
Following the first stage of re-analysis, `r nrow(df_to_rev)` meta-analyses were re-assessed because they were labelled as numerical error and/or decision error.
```{r second_stage, include=FALSE}

rm(list=ls())
source(here::here("analysis", "03_second_stage.R"))

```
Of these, `r nrow(df_to_reanalyse)` were re-analysed again as some coding errors were found in the second stage. After this, `r nrow(filter(df_to_reanalyse, numerical_error != "error"))` were re-labelled as reproduced and `r nrow(filter(df_to_reanalyse, numerical_error == "error"))` still had relevant discrepancies. Furthermore, `r nrow(filter(df_mas_revised, qualitative_check == "rep"))` were labelled as \textcolor{blue}{approximately reproduced or reproduced with minor adjustment} in a qualitative check because the discrepancy was probably explained by rounding issues, inverted signs for results (when effect sizes were reported in absolute values) and primary data, minor reporting errors, or minor adjustments in the analytical scheme^[Full details in Supplementary File at: https://osf.io/fjhpw]. In the remaining `r nrow(filter(df_mas_revised, qualitative_check == "request_information"))`, and in the `r nrow(filter(df_to_reanalyse, numerical_error == "error"))` re-analysed again without success, some issues or relevant discrepancies without apparent explanation were found. 
Figure 5 displays a scatterplot showing the consistency between the original and reproduced summary effect size and their confidence bounds of these `r nrow(df_to_scatter)` meta-analyses. 
Additionally, as a secondary analysis, the reproducibility of the $I^2$ heterogeneity statistic was explored. Figure 6 displays a scatterplot showing the consistency between the original and reproduced $I^2$ statistics. As shown in Figures 5 and 6, the discrepancies found in the heterogeneity statistic $I^2$ are larger than those found in the summary effects and their confidence intervals. \textcolor{blue}{The Pearson's correlation between the summary effect and $I^2$ discrepancies was .172.} The lack of precision of the available data (rounded data) or incomplete information on aspects such as the tau-squared estimator applied seem to have a substantial impact on the reproducibility of this result.

``` {r, fig.width = 4, fig.height=8, fig.cap="Scatterplot displaying the reproduced values as a function of the original values classified by whether or not decision error was found. Only the results of the 52 meta-analyses with a discrepancy of more than 5% identified in the first stage are displayed, but with the corrections made in the second stage. In panel (a) the summary effects are displayed and in panel (b) the confidence intervals. For (b) the colours represent lower or upper bound of the confidence interval."}
figure5

```

``` {r, fig.width = 4.5, fig.height=4.5, fig.cap="Scatterplot displaying the reproduced values as a function of the original values classified by whether or not decision error was found. Only the results of the 52 meta-analyses with a discrepancy of more than 5% identified in the first stage are displayed, but with the corrections made in the second stage. The values displayed are $I^2$ heterogeneity statistics. The size of the crosses is a function of the discrepancy in the summary effect."}
figure6

```

## Main issues identified 
Different issues in these `r nrow(df_to_rev2)` meta-analyses were identified in the second stage. For example, for one of the meta-analyses which showed a discrepancy in the confidence limits, inconsistencies were found in the original meta-analytic report itself. The confidence limits originally reported for that meta-analysis were different in the abstract, main text and forest plot. Matching the reproduced results were those reported in the forest plot but not those reported in the text. Furthermore, inconsistencies in the original summary effect reported were found between the results reported in abstract and the results reported in the main text and the forest plot. Also, in a paper where primary data were available in both a table and a forest plot, minor inconsistencies were found between the primary data of the table and the forest plot. \textcolor{blue}{These examples of inconsistencies in original results or data were found in 4 cases (3\%, 4/146).} These appeared to be typos. 
Furthermore, some inconsistencies were found with respect to the number of primary studies included in each meta-analysis. For example, in one of the meta-analyses, the main text reported the inclusion of 10 comparisons in the meta-analysis, whereas in a table of results 11 comparisons were reported for this meta-analysis. On the other hand, in 11 meta-analyses the primary data retrieved from the supplementary materials were not sufficient to reach the number of primary studies stated as included in this meta-analysis in the original report.

```{r third_stage, include=FALSE}

rm(list=ls())
source(here::here("analysis", "04_third_stage.R"))

```

## Original authors clarifications
These `r nrow(df_to_rev2)` meta-analyses were from `r sum(df_success_request$n)` different papers. Therefore, `r sum(df_success_request$n)` clarification requests with information about the study aims, methods, and preliminary results were sent to the corresponding authors of the original articles. A reply was received in only `r df_success_request$n[2]`  of the `r sum(df_success_request$n)` cases.
In one of them, the original authors sent back a link to an OSF repository^[According to the repository timeline the project was created on 02/06/2019 and according to the journal's article history the paper was published on 13/06/2019. It seems that the repository was created as a journal requirement] where the original data and analysis script were stored. According to the authors, this link was not reported in the paper by mistake. The script was run on these data and the results were successfully reproduced. In this case, the data previously used were retrieved from a forest plot (means and standard deviations) and a table (sample sizes) reported in the paper. The previous discrepancy was explained by two cases included in the original meta-analysis from the same primary study that were reported with the same ID in the forest plot and were not correctly matched with their corresponding sample size extracted from the table. This situation exemplifies the potential issues arising from having to reconstruct the original data from tables and figures and not having open access to the original data file. 

In the other case, the original data was retrieved from a huge table in supplementary material with all effect sizes and their confidence limits. The original authors sent back this same table by increasing the number of decimal places of the effect sizes and after correcting some wrong values that they themselves detected in that process. This fixed the discrepancies for some of the meta-analyses in this paper.


# Discussion 
The main aim of this study was to examine the reproducibility of a sample of published meta-analyses on the effectiveness of clinical psychology interventions. We analyzed the availability and reusability of original data and, assessed the reproducibility of the published results using these retrieved original data, and tried to reconstruct the original analysis plan. We encountered both difficulties in retrieving the original data and some problems with the reproducibility of the meta-analyses examined.

Even when we interpret data availability in the broad sense (i.e. retrieving data from tables and figures when no data file was available), for about a third of the included meta-analyses no data were available.  In these cases, attempts were made to obtain the data on request to the corresponding author, with little success. Authors only shared data in 12% of the requests that were made. This result is in line with what was found in a recent study where data availability statements from a set of primary studies were analysed [@gabelica2022]. Although 42% of primary studies in @gabelica2022 reported data were available on request (an identical percentage was found in @page2022 for meta-analyses), only 6.8% of the authors shared the underlying data when requested. Even though it is common to see authors state data is available on request, actually obtaining the data on request seems highly challenging. Although this problem of retrieving data on request is well known [@wicherts2006], the situation does not seem to have improved. Nowadays, there are straightforward, free, and open ways to share data, including meta-analytic data files. Several repositories (e.g., OSF, GitHub, Zenodo, Figshare) are available for researchers to openly share the data associated with published results. On-request availability has proven to be inadequate, and with the availability of data repositories it is no longer necessary. Journals publishing meta-analyses should require that authors share the underlying data in a public data repository. 

\textcolor{blue}{A more positive sign comes from the positive association between publication year and the possibility of retrieving the data that was observed in an exploratory analysis in the supplementary materials. This tentatively suggests that data-availability is improving over time. This observation could be related to the existence of well-established meta-analysis reporting guidelines. For instance, the first PRISMA guideline} [@moher2009] \textcolor{blue}{encouraged meta-analyst to report results of primary studies (e.g. primary effect sizes and their confidence interval through a forest plot, as was a common scenario among the cases included in this project), and the latest PRISMA guideline} [@page2021] \textcolor{blue}{which puts more emphasis on appropriate data sharing through data files ready for reuse.} At the same time in only 5% of the cases where data were retrieved in our sample were we able to retrieve the data in a machine-readable data file that was ready for reuse (e.g., *csv*, *xlsx*). Most often the data had to be retrieved from files in document format (e.g., *docx*, *pdf*). This forces people who want to reuse the data to manually recode the data, which is an inefficient and error-prone task. Even after partial double coding was carried out, this procedure did not avoid some coding errors, which were only detected by double checking meta-analyses with discrepancies. In our experience, the data retrieval process can be difficult when results are presented in general tables, as it involves matching subsets of these primary data with different meta-analytic results, while is not always clear which studies were used in which meta-analysis reported in a paper. Furthermore, because the tables in manuscript are often generated manually in document file formats (e.g. Word), we observed examples where this introduced another source of error. \textcolor{blue}{The foregoing discussion raises a key point about how time consuming the appraisal of meta-analytic reproducibility currently is, and how efficiency would be improved by having open access to the underlying meta-analytic data in data file formats ready for reuse. The latest PRISMA guidelines, along with some initiatives that promote appropriate data sharing} [e.g. @wilkinson2016] \textcolor{blue}{have the potential to generate significant improvements in the re-use of meta-analytic data in the years ahead. In this regard, our results provide a useful baseline for future assessments.}

An important finding is that the availability of the original analysis script was very limited. Only in five meta-analyses (3%, all from the same paper), was the original script openly available. In most cases, the original analyses were reconstructed from the description provided in the paper itself, which was not always rich in detail, so many of these computational details had to be inferred from the default settings of the software authors used. The availability of analysis scripts often shows similar rates, both in meta-analyses [@page2022; @polanin2020] and in primary research [@hardwicke2020; @hardwicke2022]. This makes it more difficult to easily check the computational reproducibility of the results from such studies. Reconstructing the analytical scheme adds to the workload, with the potential to introduce errors, both in the original report and in the reconstruction, and deals with the eventual lack of relevant analytical information. With the increasing availability of excellent open-source tools to perform meta-analysis (e.g., *metafor* [@viechtbauer2010] in R) and useful templates [@moreau2020], meta-analysts can use workflows that allow them to create and share analysis code for meta-analyses.

Despite these difficulties, we were able to recover the original data and reconstruct the original analysis approach, for 146 meta-analyses, for which the reproducibility of the results was assessed. These attempts went through several stages as explained above, trying to minimize the impact of possible coding errors, and requesting clarifications from the original authors. Nevertheless, even with these efforts, some discrepancies remained in the results. We identified different issues that hindered our reproducibility attempts. For example, in some cases internal discrepancies were found in the paper itself (e.g., text-figure discrepancies, text-abstract, or text-table discrepancies). Furthermore, some problems were found with the lack of some primary data, where data available in the supplementary material included fewer cases than those finally reported in the results of the published paper. These situations could be explained by typos in the manuscript, or updates when performing the meta-analysis that produced different versions of the manuscript, data, or supplementary material. While it is important to note that discrepancies in the summary effect results and their confidence intervals were mostly minor, with little or no impact on the conclusions, these situations are easily avoidable. Some of the problems identified could be explained by typos. Currently, there are tools that facilitate the production of so-called reproducible manuscripts, such as the R packages *knitr* [@xie2022], *rmarkdown* [@allaire2022], and *papaja* [@aust2022]. A reproducible manuscript embeds analysis code, data and results reporting in a single document, extracting and reporting the results from the output of the computational process itself, avoiding error-prone manual transcriptions. 

Our results are complementary to those observed in previous research on the reproducibility of the primary effects of meta-analyses [@gotzsche2007; @maassen2020] and related problems due to the multiplicity of primary effects [@tendal2009]. These studies found problems in reproducing the primary effects of published meta-analyses, or in reaching agreement between independent coders in computing them. Such problems, to a greater or lesser extent, had some impact on the meta-analytic results. Our results show that, even when re-using the primary effects as originally coded, certain problems of reproducibility of the results may remain. Some of these problems are added error on the source of error found in previous research on reproducibility of primary effects, which in turn are added error on the sources of error types of primary estimates (e.g., measurement error, sampling error, or reporting errors). No scientific research is totally error-free, but one of the main tasks of scientists is to minimize this error, and in some cases, such as those observed in this study, minimizing some potential sources of error can be straightforward.  

Our study has some limitations. First, the time span covered is fairly wide. Thus, the findings may not capture the changes that have arisen in recent years. Therefore, future studies should examine more specific changes over years, to evaluate whether better practices emerge that facilitate reproducibility. Second, most of the primary data was retrieved through manual re-coding, which introduces some error. The reported data was rounded, which means we did not have access to precise values, and in many cases the standard error had to be approximated from the confidence limits. These limitation of our study are caused by the suboptimal practices when sharing data we discussed above. Given the non-precise nature of most of the data retrieved, we had to make a decision about which margin of discrepancy was acceptable. In this study, a margin of 5% was chosen. Because this cut-off is arbitrary, we have tried to focus more on possible issues in the results that fell above this margin, than on establishing a exact ratio of non-reproduced meta-analyses based on this arbitrary cut-off. \textcolor{blue}{Finally, we only examined meta-analyses in clinical psychology as this is one of the areas that produces the most meta-analyses in psychology and these meta-analyses have a direct impact on applied practice, but it is unknown to which extent our conclusions generalize to meta-analyses in other sub-disciplines in psychology.}

In conclusion, we observed several difficulties when attempting to reproduce meta-analyses. Two aspects can be highlighted: (1) data availability and reusability of the data as they are shared, (2) and apparent errors in the reporting of results. As data collected for a meta-analysis can be especially useful for future research, direct and open access to such datasets allows for easy updates, and re-analyses, which are valuable in a cumulative science. Meta-analytic data generally do not contain sensitive or personal information, and can therefore almost always be shared openly, as doing so does not involve ethical or legal conflicts. Third, meta-analytic results often represent the state of the art of the evidence on a particular topic. These results guide applied practice, public policy, or future research directions. This prominent status entails a major responsibility for the credibility, reliability, and validity of published meta-analytic results. 

## Author Contributions
Conceptualization: R. Lopez-Nicolas and J. Sanchez-Meca; Methodology: R. Lopez-Nicolas, D. Lakens, J.A. Lopez-Lopez and J. Sanchez-Meca; Formal Analysis: R Lopez-Nicolas; Investigation: R Lopez-Nicolas, M. Rubio-Aparicio, A. Sandoval-Lentisco, C. Lopez-Ibañez and D. Blazquez-Rincon: Data curation: R Lopez-Nicolas; Writing – Original Draft Preparation: R. Lopez-Nicolas; Writing – Review & Editing: D. Lakens, J.A. Lopez-Lopez, J. Sanchez-Meca, M. Rubio-Aparicio, A. Sandoval-Lentisco, C. Lopez-Ibañez, D. Blazquez-Rincon. 

## Conflict of interest
The author(s) declare that there were no conflicts of interest with respect to the authorship or the publication of this article.

## Acknowledgments 
We thank the research group of the Meta-Research Centre of Tilburg University for providing feedback on an earlier version of this manuscript. We would also like to thank all the original authors of the included meta-analyses who enabled access to the data used in this project as well as the authors who provided clarifications in response to requests for clarification in the third stage. 

## Funding 
This research has been funded with a grant from the Spanish Ministry of Science and Innovation (Project PID2019-104080GB-I00/AEI/10.13039/501100011033, FEDER funds) and by the Spanish Ministry of Universities (predoctoral grant: FPU18/04805).

## Supplemental Material

Supplementary material available at: https://osf.io/fjhpw

## Prior versions

A version of this manuscript has been posted as a preprint on PsyArxiv: https://psyarxiv.com/gvqrn/



# References