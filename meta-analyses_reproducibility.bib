  @Manual{allaire2022,
    title = {rmarkdown: Dynamic Documents for R},
    author = {JJ Allaire and Yihui Xie and Jonathan McPherson and
      Javier Luraschi and Kevin Ushey and Aron Atkins and Hadley
      Wickham and Joe Cheng and Winston Chang and Richard Iannone},
    year = {2022},
    note = {R package version 2.14},
    url = {https://github.com/rstudio/rmarkdown},
  }

@article{artner2020,
  title = {The Reproducibility of Statistical Results in Psychological Research: {{An}} Investigation Using Unpublished Raw Data},
  shorttitle = {The Reproducibility of Statistical Results in Psychological Research},
  author = {Artner, Richard and Verliefde, Thomas and Steegen, Sara and Gomes, Sara and Traets, Frits and Tuerlinckx, Francis and Vanpaemel, Wolf},
  year = {2020},
  journal = {Psychological Methods},
  pages = {No Pagination Specified-No Pagination Specified},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-1463},
  doi = {10.1037/met0000365},
  abstract = {We investigated the reproducibility of the major statistical conclusions drawn in 46 articles published in 2012 in three APA journals. After having identified 232 key statistical claims, we tried to reproduce, for each claim, the test statistic, its degrees of freedom, and the corresponding p value, starting from the raw data that were provided by the authors and closely following the Method section in the article. Out of the 232 claims, we were able to successfully reproduce 163 (70\%), 18 of which only by deviating from the article's analytical description. Thirteen (7\%) of the 185 claims deemed significant by the authors are no longer so. The reproduction successes were often the result of cumbersome and time-consuming trial-and-error work, suggesting that APA style reporting in conjunction with raw data makes numerical verification at least hard, if not impossible. This article discusses the types of mistakes we could identify and the tediousness of our reproduction efforts in the light of a newly developed taxonomy for reproducibility. We then link our findings with other findings of empirical research on this topic, give practical recommendations on how to achieve reproducibility, and discuss the challenges of large-scale reproducibility checks as well as promising ideas that could considerably increase the reproducibility of psychological research. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  keywords = {American Psychological Association,Data Sets,Errors,Experimental Replication,Experimentation,Scientific Communication,Statistical Analysis,Statistics}
}



@article{bornmann2021,
  title = {Growth Rates of Modern Science: A Latent Piecewise Growth Curve Approach to Model Publication Numbers from Established and New Literature Databases},
  shorttitle = {Growth Rates of Modern Science},
  author = {Bornmann, Lutz and Haunschild, Robin and Mutz, R{\"u}diger},
  year = {2021},
  month = oct,
  journal = {Humanities and Social Sciences Communications},
  volume = {8},
  number = {1},
  pages = {1--15},
  publisher = {{Palgrave}},
  issn = {2662-9992},
  doi = {10.1057/s41599-021-00903-w},
  abstract = {Growth of science is a prevalent issue in science of science studies. In recent years, two new bibliographic databases have been introduced, which can be used to study growth processes in science from centuries back: Dimensions from Digital Science and Microsoft Academic. In this study, we used publication data from these new databases and added publication data from two established databases (Web of Science from Clarivate Analytics and Scopus from Elsevier) to investigate scientific growth processes from the beginning of the modern science system until today. We estimated regression models that included simultaneously the publication counts from the four databases. The results of the unrestricted growth of science calculations show that the overall growth rate amounts to 4.10\% with a doubling time of 17.3 years. As the comparison of various segmented regression models in the current study revealed, models with four or five segments fit the publication data best. We demonstrated that these segments with different growth rates can be interpreted very well, since they are related to either phases of economic (e.g., industrialization) and/or political developments (e.g., Second World War). In this study, we additionally analyzed scientific growth in two broad fields (Physical and Technical Sciences as well as Life Sciences) and the relationship of scientific and economic growth in UK. The comparison between the two fields revealed only slight differences. The comparison of the British economic and scientific growth rates showed that the economic growth rate is slightly lower than the scientific growth rate.},
  copyright = {2021 The Author(s)},
  langid = {english},
  keywords = {Science,Sociology,technology and society},
  annotation = {Bandiera\_abtest: a Cc\_license\_type: cc\_by Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Science, technology and society;Sociology Subject\_term\_id: science-technology-and-society;sociology},
  file = {C\:\\Users\\neeeb\\Zotero\\storage\\Z8AXIFKF\\Bornmann et al. - 2021 - Growth rates of modern science a latent piecewise.pdf;C\:\\Users\\neeeb\\Zotero\\storage\\78A9YB82\\s41599-021-00903-w.html}
}


@article{camerer2018,
  title = {Evaluating the Replicability of Social Science Experiments in {{Nature}} and {{Science}} between 2010 and 2015},
  author = {Camerer, Colin F. and Dreber, Anna and Holzmeister, Felix and Ho, Teck-Hua and Huber, J{\"u}rgen and Johannesson, Magnus and Kirchler, Michael and Nave, Gideon and Nosek, Brian A. and Pfeiffer, Thomas and Altmejd, Adam and Buttrick, Nick and Chan, Taizan and Chen, Yiling and Forsell, Eskil and Gampa, Anup and Heikensten, Emma and Hummer, Lily and Imai, Taisuke and Isaksson, Siri and Manfredi, Dylan and Rose, Julia and Wagenmakers, Eric-Jan and Wu, Hang},
  year = {2018},
  month = sep,
  journal = {Nature Human Behaviour},
  volume = {2},
  number = {9},
  pages = {637--644},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  doi = {10.1038/s41562-018-0399-z},
  abstract = {Being able to replicate scientific findings is crucial for scientific progress1\textendash 15. We replicate 21 systematically selected experimental studies in the social sciences published in Nature and Science between 2010 and 201516\textendash 36. The replications follow analysis plans reviewed by the original authors and pre-registered prior to the replications. The replications are high powered, with sample sizes on average about five times higher than in the original studies. We find a significant effect in the same direction as the original study for 13 (62\%) studies, and the effect size of the replications is on average about 50\% of the original effect size. Replicability varies between 12 (57\%) and 14 (67\%) studies for complementary replicability indicators. Consistent with these results, the estimated true-positive rate is 67\% in a Bayesian analysis. The relative effect size of true positives is estimated to be 71\%, suggesting that both false positives and inflated effect sizes of true positives contribute to imperfect reproducibility. Furthermore, we find that peer beliefs of replicability are strongly related to replicability, suggesting that the research community could predict which results would replicate and that failures to replicate were not the result of chance alone.},
  copyright = {2018 The Author(s)},
  langid = {english},
  keywords = {Economics,Psychology},
  file = {C\:\\Users\\neeeb\\Zotero\\storage\\9EA5NZHG\\Camerer et al. - 2018 - Evaluating the replicability of social science exp.pdf;C\:\\Users\\neeeb\\Zotero\\storage\\DCRR65EE\\s41562-018-0399-z.html}
}


@article{errington2021a,
  title = {Investigating the Replicability of Preclinical Cancer Biology},
  author = {Errington, Timothy M and Mathur, Maya and Soderberg, Courtney K and Denis, Alexandria and Perfito, Nicole and Iorns, Elizabeth and Nosek, Brian A},
  editor = {Pasqualini, Renata and Franco, Eduardo},
  year = {2021},
  month = dec,
  journal = {eLife},
  volume = {10},
  pages = {e71601},
  publisher = {{eLife Sciences Publications, Ltd}},
  issn = {2050-084X},
  doi = {10.7554/eLife.71601},
  abstract = {Replicability is an important feature of scientific research, but aspects of contemporary research culture, such as an emphasis on novelty, can make replicability seem less important than it should be. The Reproducibility Project: Cancer Biology was set up to provide evidence about the replicability of preclinical research in cancer biology by repeating selected experiments from high-impact papers. A total of 50 experiments from 23 papers were repeated, generating data about the replicability of a total of 158 effects. Most of the original effects were positive effects (136), with the rest being null effects (22). A majority of the original effect sizes were reported as numerical values (117), with the rest being reported as representative images (41). We employed seven methods to assess replicability, and some of these methods were not suitable for all the effects in our sample. One method compared effect sizes: for positive effects, the median effect size in the replications was 85\% smaller than the median effect size in the original experiments, and 92\% of replication effect sizes were smaller than the original. The other methods were binary \textendash{} the replication was either a success or a failure \textendash{} and five of these methods could be used to assess both positive and null effects when effect sizes were reported as numerical values. For positive effects, 40\% of replications (39/97) succeeded according to three or more of these five methods, and for null effects 80\% of replications (12/15) were successful on this basis; combining positive and null effects, the success rate was 46\% (51/112). A successful replication does not definitively confirm an original finding or its theoretical interpretation. Equally, a failure to replicate does not disconfirm a finding, but it does suggest that additional investigation is needed to establish its reliability.},
  keywords = {credibility,meta-analysis,replication,reproducibility,reproducibility in cancer biology,Reproducibility Project: Cancer Biology,transparency},
  file = {C\:\\Users\\neeeb\\Zotero\\storage\\SP7UWH9S\\Errington et al. - 2021 - Investigating the replicability of preclinical can.pdf}
}

@article{gabelica2022,
  title = {Many Researchers Were Not Compliant with Their Published Data Sharing Statement: Mixed-Methods Study},
  shorttitle = {Many Researchers Were Not Compliant with Their Published Data Sharing Statement},
  author = {Gabelica, Mirko and Boj{\v c}i{\'c}, Ru{\v z}ica and Puljak, Livia},
  year = {2022},
  month = may,
  journal = {Journal of Clinical Epidemiology},
  issn = {0895-4356},
  doi = {10.1016/j.jclinepi.2022.05.019},
  abstract = {Objectives To analyse researchers' compliance with their Data Availability Statement (DAS) from manuscripts published in open access journals with the mandatory DAS. Study Design and Setting We analyzed all articles from 333 open-access journals published during January 2019 by BioMed Central. We categorized types of DAS. We surveyed corresponding authors who wrote in DAS that they would share the data. A consent to participate in the study was sought for all included manuscripts. After accessing raw data sets, we checked whether data were available in a way that enabled re-analysis. Results Of 3556 analyzed articles, 3416 contained DAS. The most frequent DAS category (42\%) indicated that the datasets are available on reasonable request. Among 1792 manuscripts in which DAS indicated that authors are willing to share their data, 1670 (93\%) authors either did not respond or declined to share their data with us. Among 254 (14\%) of 1792 authors who responded to our query for data sharing, only 122 (6.8\%) provided the requested data. Conclusion Even when authors indicate in their manuscript that they will share data upon request, the compliance rate is the same as for authors who do not provide DAS, suggesting that DAS may not be sufficient to ensure data sharing.},
  langid = {english},
  keywords = {Data availability statement,data sharing,non-compliance,open data},
  file = {C\:\\Users\\neeeb\\Zotero\\storage\\UA53VFJX\\Gabelica et al. - 2022 - Many researchers were not compliant with their pub.pdf;C\:\\Users\\neeeb\\Zotero\\storage\\XHYBCAK6\\S089543562200141X.html}
}


@article{gotzsche2007,
  title = {Data {{Extraction Errors}} in {{Meta-analyses That Use Standardized Mean Differences}}},
  author = {G{\o}tzsche, Peter C. and Hr{\'o}bjartsson, Asbj{\o}rn and Mari{\'c}, Katja and Tendal, Britta},
  year = {2007},
  month = jul,
  journal = {JAMA},
  volume = {298},
  number = {4},
  pages = {430--437},
  issn = {0098-7484},
  doi = {10.1001/jama.298.4.430},
  abstract = {ContextMeta-analysis of trials that have used different continuous or rating scales to record outcomes of a similar nature requires sophisticated data handling and data transformation to a uniform scale, the standardized mean difference (SMD). It is not known how reliable such meta-analyses are.ObjectiveTo study whether SMDs in meta-analyses are accurate.Data SourcesSystematic review of meta-analyses published in 2004 that reported a result as an SMD, with no language restrictions. Two trials were randomly selected from each meta-analysis. We attempted to replicate the results in each meta-analysis by independently calculating SMD using Hedges adjusted g.Data ExtractionOur primary outcome was the proportion of meta-analyses for which our result differed from that of the authors by 0.1 or more, either for the point estimate or for its confidence interval, for at least 1 of the 2 selected trials. We chose 0.1 as cut point because many commonly used treatments have an effect of 0.1 to 0.5, compared with placebo.ResultsOf the 27 meta-analyses included in this study, we could not replicate the result for at least 1 of the 2 trials within 0.1 in 10 of the meta-analyses (37\%), and in 4 cases, the discrepancy was 0.6 or more for the point estimate. Common problems were erroneous number of patients, means, standard deviations, and sign for the effect estimate. In total, 17 meta-analyses (63\%) had errors for at least 1 of the 2 trials examined. For the 10 meta-analyses with errors of at least 0.1, we checked the data from all the trials and conducted our own meta-analysis, using the authors' methods. Seven of these 10 meta-analyses were erroneous (70\%); 1 was subsequently retracted, and in 2 a significant difference disappeared or appeared.ConclusionsThe high proportion of meta-analyses based on SMDs that show errors indicates that although the statistical process is ostensibly simple, data extraction is particularly liable to errors that can negate or even reverse the findings of the study. This has implications for researchers and implies that all readers, including journal reviewers and policy makers, should approach such meta-analyses with caution.}
}


@article{gurevitch2018,
  title = {Meta-Analysis and the Science of Research Synthesis},
  author = {Gurevitch, Jessica and Koricheva, Julia and Nakagawa, Shinichi and Stewart, Gavin},
  year = {2018},
  month = mar,
  journal = {Nature},
  volume = {555},
  number = {7695},
  pages = {175--182},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/nature25753},
  abstract = {Meta-analysis is the quantitative, scientific synthesis of research results. Since the term and modern approaches to research synthesis were first introduced in the 1970s, meta-analysis has had a revolutionary effect in many scientific fields, helping to establish evidence-based practice and to resolve seemingly contradictory research outcomes. At the same time, its implementation has engendered criticism and controversy, in some cases general and others specific to particular disciplines. Here we take the opportunity provided by the recent fortieth anniversary of meta-analysis to reflect on the accomplishments, limitations, recent advances and directions for future developments in the field of research synthesis.},
  copyright = {2018 Macmillan Publishers Limited, part of Springer Nature. All rights reserved.},
  langid = {english},
  keywords = {Biodiversity,Outcomes research},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Reviews Subject\_term: Biodiversity;Outcomes research Subject\_term\_id: biodiversity;outcomes-research},
  file = {C\:\\Users\\neeeb\\Zotero\\storage\\RMPXSGX2\\nature25753.html}
}

@article{hardwicke2018,
  title = {Data Availability, Reusability, and Analytic Reproducibility: Evaluating the Impact of a Mandatory Open Data Policy at the Journal {{Cognition}}},
  shorttitle = {Data Availability, Reusability, and Analytic Reproducibility},
  author = {Hardwicke, Tom E. and Mathur, Maya B. and MacDonald, Kyle and Nilsonne, Gustav and Banks, George C. and Kidwell, Mallory C. and Hofelich Mohr, Alicia and Clayton, Elizabeth and Yoon, Erica J. and Henry Tessler, Michael and Lenne, Richie L. and Altman, Sara and Long, Bria and Frank, Michael C.},
  year = {2018},
  month = aug,
  journal = {Royal Society Open Science},
  volume = {5},
  number = {8},
  pages = {180448},
  issn = {2054-5703},
  doi = {10.1098/rsos.180448},
  abstract = {Access to data is a critical feature of an efficient, progressive and ultimately self-correcting scientific ecosystem. But the extent to which in-principle benefits of data sharing are realized in practice is unclear. Crucially, it is largely unknown whether published findings can be reproduced by repeating reported analyses upon shared data ('analytic reproducibility'). To investigate this, we conducted an observational evaluation of a mandatory open data policy introduced at the journal Cognition. Interrupted time-series analyses indicated a substantial post-policy increase in data available statements (104/417, 25\% pre-policy to 136/174, 78\% post-policy), although not all data appeared reusable (23/104, 22\% pre-policy to 85/136, 62\%, post-policy). For 35 of the articles determined to have reusable data, we attempted to reproduce 1324 target values. Ultimately, 64 values could not be reproduced within a 10\% margin of error. For 22 articles all target values were reproduced, but 11 of these required author assistance. For 13 articles at least one value could not be reproduced despite author assistance. Importantly, there were no clear indications that original conclusions were seriously impacted. Mandatory open data policies can increase the frequency and quality of data sharing. However, suboptimal data curation, unclear analysis specification and reporting errors can impede analytic reproducibility, undermining the utility of data sharing and the credibility of scientific findings.},
  langid = {english},
  pmcid = {PMC6124055},
  pmid = {30225032},
  keywords = {interrupted time series,journal policy,meta-science,open data,open science,reproducibility},
  file = {C\:\\Users\\neeeb\\Zotero\\storage\\VSJVKCZL\\Hardwicke et al. - 2018 - Data availability, reusability, and analytic repro.pdf}
}

@article{hardwicke2020,
  title = {An Empirical Assessment of Transparency and Reproducibility-Related Research Practices in the Social Sciences (2014-2017)},
  author = {Hardwicke, Tom E. and Wallach, Joshua D. and Kidwell, Mallory C. and Bendixen, Theiss and Cr{\"u}well, Sophia and Ioannidis, John P. A.},
  year = {2020},
  month = feb,
  journal = {Royal Society Open Science},
  volume = {7},
  number = {2},
  pages = {190806},
  issn = {2054-5703},
  doi = {10.1098/rsos.190806},
  abstract = {Serious concerns about research quality have catalysed a number of reform initiatives intended to improve transparency and reproducibility and thus facilitate self-correction, increase efficiency and enhance research credibility. Meta-research has evaluated the merits of some individual initiatives; however, this may not capture broader trends reflecting the cumulative contribution of these efforts. In this study, we manually examined a random sample of 250 articles in order to estimate the prevalence of a range of transparency and reproducibility-related indicators in the social sciences literature published between 2014 and 2017. Few articles indicated availability of materials (16/151, 11\% [95\% confidence interval, 7\% to 16\%]), protocols (0/156, 0\% [0\% to 1\%]), raw data (11/156, 7\% [2\% to 13\%]) or analysis scripts (2/156, 1\% [0\% to 3\%]), and no studies were pre-registered (0/156, 0\% [0\% to 1\%]). Some articles explicitly disclosed funding sources (or lack of; 74/236, 31\% [25\% to 37\%]) and some declared no conflicts of interest (36/236, 15\% [11\% to 20\%]). Replication studies were rare (2/156, 1\% [0\% to 3\%]). Few studies were included in evidence synthesis via systematic review (17/151, 11\% [7\% to 16\%]) or meta-analysis (2/151, 1\% [0\% to 3\%]). Less than half the articles were publicly available (101/250, 40\% [34\% to 47\%]). Minimal adoption of transparency and reproducibility-related research practices could be undermining the credibility and efficiency of social science research. The present study establishes a baseline that can be revisited in the future to assess progress.},
  langid = {english},
  pmcid = {PMC7062098},
  pmid = {32257301},
  keywords = {meta-research,open science,reproducibility,social sciences,transparency},
  file = {C\:\\Users\\neeeb\\Zotero\\storage\\6VTTUL42\\Hardwicke et al. - 2020 - An empirical assessment of transparency and reprod.pdf}
}

@article{hardwicke2021,
  title = {Analytic Reproducibility in Articles Receiving Open Data Badges at the Journal {{Psychological Science}}: An Observational Study},
  shorttitle = {Analytic Reproducibility in Articles Receiving Open Data Badges at the Journal {{Psychological Science}}},
  author = {Hardwicke, Tom E. and Bohn, Manuel and MacDonald, Kyle and Hembacher, Emily and Nuijten, Mich{\`e}le B. and Peloquin, Benjamin N. and {deMayo}, Benjamin E. and Long, Bria and Yoon, Erica J. and Frank, Michael C.},
  year = {2021},
  month = jan,
  journal = {Royal Society Open Science},
  volume = {8},
  number = {1},
  pages = {201494},
  issn = {2054-5703},
  doi = {10.1098/rsos.201494},
  abstract = {For any scientific report, repeating the original analyses upon the original data should yield the original outcomes. We evaluated analytic reproducibility in 25 Psychological Science articles awarded open data badges between 2014 and 2015. Initially, 16 (64\%, 95\% confidence interval [43,81]) articles contained at least one 'major numerical discrepancy' ({$>$}10\% difference) prompting us to request input from original authors. Ultimately, target values were reproducible without author involvement for 9 (36\% [20,59]) articles; reproducible with author involvement for 6 (24\% [8,47]) articles; not fully reproducible with no substantive author response for 3 (12\% [0,35]) articles; and not fully reproducible despite author involvement for 7 (28\% [12,51]) articles. Overall, 37 major numerical discrepancies remained out of 789 checked values (5\% [3,6]), but original conclusions did not appear affected. Non-reproducibility was primarily caused by unclear reporting of analytic procedures. These results highlight that open data alone is not sufficient to ensure analytic reproducibility.},
  langid = {english},
  pmcid = {PMC7890505},
  pmid = {33614084},
  keywords = {journal policy,meta-research,open badges,open data,open science,reproducibility},
  file = {C\:\\Users\\neeeb\\Zotero\\storage\\FTK9RTE8\\Hardwicke et al. - 2021 - Analytic reproducibility in articles receiving ope.pdf}
}

@article{hardwicke2022,
  title = {Estimating the {{Prevalence}} of {{Transparency}} and {{Reproducibility-Related Research Practices}} in {{Psychology}} (2014\textendash 2017)},
  author = {Hardwicke, Tom E. and Thibault, Robert T. and Kosie, Jessica E. and Wallach, Joshua D. and Kidwell, Mallory C. and Ioannidis, John P. A.},
  year = {2022},
  month = jan,
  journal = {Perspectives on Psychological Science},
  volume = {17},
  number = {1},
  pages = {239--251},
  publisher = {{SAGE Publications Inc}},
  issn = {1745-6916},
  doi = {10.1177/1745691620979806},
  abstract = {Psychologists are navigating an unprecedented period of introspection about the credibility and utility of their discipline. Reform initiatives emphasize the benefits of transparency and reproducibility-related research practices; however, adoption across the psychology literature is unknown. Estimating the prevalence of such practices will help to gauge the collective impact of reform initiatives, track progress over time, and calibrate future efforts. To this end, we manually examined a random sample of 250 psychology articles published between 2014 and 2017. Over half of the articles were publicly available (154/237, 65\%, 95\% confidence interval [CI] = [59\%, 71\%]); however, sharing of research materials (26/183; 14\%, 95\% CI = [10\%, 19\%]), study protocols (0/188; 0\%, 95\% CI = [0\%, 1\%]), raw data (4/188; 2\%, 95\% CI = [1\%, 4\%]), and analysis scripts (1/188; 1\%, 95\% CI = [0\%, 1\%]) was rare. Preregistration was also uncommon (5/188; 3\%, 95\% CI = [1\%, 5\%]). Many articles included a funding disclosure statement (142/228; 62\%, 95\% CI = [56\%, 69\%]), but conflict-of-interest statements were less common (88/228; 39\%, 95\% CI = [32\%, 45\%]). Replication studies were rare (10/188; 5\%, 95\% CI = [3\%, 8\%]), and few studies were included in systematic reviews (21/183; 11\%, 95\% CI = [8\%, 16\%]) or meta-analyses (12/183; 7\%, 95\% CI = [4\%, 10\%]). Overall, the results suggest that transparency and reproducibility-related research practices were far from routine. These findings establish baseline prevalence estimates against which future progress toward increasing the credibility and utility of psychology research can be compared.},
  langid = {english},
  keywords = {meta-research,open science,psychology,reproducibility,transparency},
  file = {C\:\\Users\\neeeb\\Zotero\\storage\\CBCM8ART\\Hardwicke et al. - 2022 - Estimating the Prevalence of Transparency and Repr.pdf}
}


@article{klein2014a,
  title = {Investigating {{Variation}} in {{Replicability}}},
  author = {Klein, Richard A. and Ratliff, Kate A. and Vianello, Michelangelo and Adams, Reginald B. and Bahn{\'i}k, {\v S}t{\v e}p{\'a}n and Bernstein, Michael J. and Bocian, Konrad and Brandt, Mark J. and Brooks, Beach and Brumbaugh, Claudia Chloe and Cemalcilar, Zeynep and Chandler, Jesse and Cheong, Winnee and Davis, William E. and Devos, Thierry and Eisner, Matthew and Frankowska, Natalia and Furrow, David and Galliani, Elisa Maria and Hasselman, Fred and Hicks, Joshua A. and Hovermale, James F. and Hunt, S. Jane and Huntsinger, Jeffrey R. and IJzerman, Hans and John, Melissa-Sue and {Joy-Gaba}, Jennifer A. and Barry Kappes, Heather and Krueger, Lacy E. and Kurtz, Jaime and Levitan, Carmel A. and Mallett, Robyn K. and Morris, Wendy L. and Nelson, Anthony J. and Nier, Jason A. and Packard, Grant and Pilati, Ronaldo and Rutchick, Abraham M. and Schmidt, Kathleen and Skorinko, Jeanine L. and Smith, Robert and Steiner, Troy G. and Storbeck, Justin and Van Swol, Lyn M. and Thompson, Donna and {van `t Veer}, A. E. and Ann Vaughn, Leigh and Vranka, Marek and Wichman, Aaron L. and Woodzicka, Julie A. and Nosek, Brian A.},
  year = {2014},
  month = may,
  journal = {Social Psychology},
  volume = {45},
  number = {3},
  pages = {142--152},
  publisher = {{Hogrefe Publishing}},
  issn = {1864-9335},
  doi = {10.1027/1864-9335/a000178},
  abstract = {Although replication is a central tenet of science, direct replications are rare in psychology. This research tested variation in the replicability of 13 classic and contemporary effects across 36 independent samples totaling 6,344 participants. In the aggregate, 10 effects replicated consistently. One effect \textendash{} imagined contact reducing prejudice \textendash{} showed weak support for replicability. And two effects \textendash{} flag priming influencing conservatism and currency priming influencing system justification \textendash{} did not replicate. We compared whether the conditions such as lab versus online or US versus international sample predicted effect magnitudes. By and large they did not. The results of this small sample of effects suggest that replicability is more dependent on the effect itself than on the sample and setting used to investigate the effect.},
  keywords = {cross-cultural,generalizability,replication,reproducibility,variation},
  file = {C\:\\Users\\neeeb\\Zotero\\storage\\MCMSQHPD\\Klein et al. - 2014 - Investigating Variation in Replicability.pdf}
}


@article{lopez-nicolas2021,
  title = {A Meta-Review of Transparency and Reproducibility-Related Reporting Practices in Published Meta-Analyses on Clinical Psychological Interventions (2000\textendash 2020)},
  author = {{L{\'o}pez-Nicol{\'a}s}, Rub{\'e}n and {L{\'o}pez-L{\'o}pez}, Jos{\'e} Antonio and {Rubio-Aparicio}, Mar{\'i}a and {S{\'a}nchez-Meca}, Julio},
  year = {2021},
  month = jun,
  journal = {Behavior Research Methods},
  issn = {1554-3528},
  doi = {10.3758/s13428-021-01644-z},
  abstract = {Meta-analysis is a powerful and important tool to synthesize the literature about a research topic. Like other kinds of research, meta-analyses must be reproducible to be compliant with the principles of the scientific method. Furthermore, reproducible meta-analyses can be easily updated with new data and reanalysed applying new and more refined analysis techniques. We attempted to empirically assess the prevalence of transparency and reproducibility-related reporting practices in published meta-analyses from clinical psychology by examining a random sample of 100 meta-analyses. Our purpose was to identify the key points that could be improved, with the aim of providing some recommendations for carrying out reproducible meta-analyses. We conducted a meta-review of meta-analyses of psychological interventions published between 2000 and 2020. We searched PubMed, PsycInfo and Web of Science databases. A structured coding form to assess transparency indicators was created based on previous studies and existing meta-analysis guidelines. We found major issues concerning: completely reproducible search procedures report, specification of the exact method to compute effect sizes, choice of weighting factors and estimators, lack of availability of the raw statistics used to compute the effect size and of interoperability of available data, and practically total absence of analysis script code sharing. Based on our findings, we conclude with recommendations intended to improve the transparency, openness, and reproducibility-related reporting practices of meta-analyses in clinical psychology and related areas.},
  langid = {english},
  file = {C\:\\Users\\neeeb\\Zotero\\storage\\8FG4BUAQ\\López-Nicolás et al. - 2021 - A meta-review of transparency and reproducibility-.pdf}
}

@article{maassen2020,
  title = {Reproducibility of Individual Effect Sizes in Meta-Analyses in Psychology},
  author = {Maassen, Esther and van Assen, Marcel A. L. M. and Nuijten, Mich{\`e}le B. and {Olsson-Collentine}, Anton and Wicherts, Jelte M.},
  year = {2020},
  month = may,
  journal = {PLOS ONE},
  volume = {15},
  number = {5},
  pages = {e0233107},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0233107},
  abstract = {To determine the reproducibility of psychological meta-analyses, we investigated whether we could reproduce 500 primary study effect sizes drawn from 33 published meta-analyses based on the information given in the meta-analyses, and whether recomputations of primary study effect sizes altered the overall results of the meta-analysis. Results showed that almost half (k = 224) of all sampled primary effect sizes could not be reproduced based on the reported information in the meta-analysis, mostly because of incomplete or missing information on how effect sizes from primary studies were selected and computed. Overall, this led to small discrepancies in the computation of mean effect sizes, confidence intervals and heterogeneity estimates in 13 out of 33 meta-analyses. We provide recommendations to improve transparency in the reporting of the entire meta-analytic process, including the use of preregistration, data and workflow sharing, and explicit coding practices.},
  langid = {english},
  keywords = {Clinical psychology,Metaanalysis,Peer review,Psychology,Publication ethics,Reproducibility,Research reporting guidelines,Systematic reviews},
  file = {C\:\\Users\\neeeb\\Zotero\\storage\\ZEHFMN8P\\Maassen et al. - 2020 - Reproducibility of individual effect sizes in meta.pdf}
}


@book{nationalacademiesofsciencesengineeringandmedicine2019,
  title = {Reproducibility and {{Replicability}} in {{Science}}},
  author = {{National Academies of Sciences, Engineering, and Medicine}},
  year = {2019},
  publisher = {{The National Academies Press}},
  address = {{Washington, DC}},
  doi = {10.17226/25303},
  abstract = {One of the pathways by which the scientific community confirms the validity of a new scientific discovery is by repeating the research that produced it. When a scientific effort fails to independently confirm the computations or results of a previous study, some fear that it may be a symptom of a lack of rigor in science, while others argue that such an observed inconsistency can be an important precursor to new discovery. Concerns about reproducibility and replicability have been expressed in both scientific and popular media. As these concerns came to light, Congress requested that the National Academies of Sciences, Engineering, and Medicine conduct a study to assess the extent of issues related to reproducibility and replicability and to offer recommendations for improving rigor and transparency in scientific research. Reproducibility and Replicability in Science defines reproducibility and replicability and examines the factors that may lead to non-reproducibility and non-replicability in research. Unlike the typical expectation of reproducibility between two computations, expectations about replicability are more nuanced, and in some cases a lack of replicability can aid the process of scientific discovery. This report provides recommendations to researchers, academic institutions, journals, and funders on steps they can take to improve reproducibility and replicability in science.},
  isbn = {978-0-309-48616-3},
  langid = {english},
  keywords = {Policy for Science and Technology,Surveys and Statistics}
}


@article{nosek2022,
  title = {Replicability, {{Robustness}}, and {{Reproducibility}} in {{Psychological Science}}},
  author = {Nosek, Brian A. and Hardwicke, Tom E. and Moshontz, Hannah and Allard, Aur{\'e}lien and Corker, Katherine S. and Dreber, Anna and Fidler, Fiona and Hilgard, Joe and Kline Struhl, Melissa and Nuijten, Mich{\`e}le B. and Rohrer, Julia M. and Romero, Felipe and Scheel, Anne M. and Scherer, Laura D. and Sch{\"o}nbrodt, Felix D. and Vazire, Simine},
  year = {2022},
  journal = {Annual Review of Psychology},
  volume = {73},
  number = {1},
  pages = {719--748},
  doi = {10.1146/annurev-psych-020821-114157},
  abstract = {Replication\textemdash an important, uncommon, and misunderstood practice\textemdash is gaining appreciation in psychology. Achieving replicability is important for making research progress. If findings are not replicable, then prediction and theory development are stifled. If findings are replicable, then interrogation of their meaning and validity can advance knowledge. Assessing replicability can be productive for generating and testing hypotheses by actively confronting current understandings to identify weaknesses and spur innovation. For psychology, the 2010s might be characterized as a decade of active confrontation. Systematic and multi-site replication projects assessed current understandings and observed surprising failures to replicate many published findings. Replication efforts highlighted sociocultural challenges such as disincentives to conduct replications and a tendency to frame replication as a personal attack rather than a healthy scientific practice, and they raised awareness that replication contributes to self-correction. Nevertheless, innovation in doing and understanding replication and its cousins, reproducibility and robustness, has positioned psychology to improve research practices and accelerate progress.},
  pmid = {34665669},
  keywords = {generalizability,metascience,replication,reproducibility,research methods,robustness,statistical inference,theory,validity},
  annotation = {\_eprint: https://doi.org/10.1146/annurev-psych-020821-114157},
  file = {C\:\\Users\\neeeb\\Zotero\\storage\\G7MZV24H\\Nosek et al. - 2022 - Replicability, Robustness, and Reproducibility in .pdf}
}


@article{opensciencecollaboration2015,
  title = {Estimating the Reproducibility of Psychological Science},
  author = {{Open Science Collaboration}},
  year = {2015},
  month = aug,
  journal = {Science},
  volume = {349},
  number = {6251},
  pages = {aac4716},
  publisher = {{American Association for the Advancement of Science}},
  doi = {10.1126/science.aac4716},
  file = {C\:\\Users\\neeeb\\Zotero\\storage\\STKPLP7C\\OPEN SCIENCE COLLABORATION - 2015 - Estimating the reproducibility of psychological sc.pdf}
}

@article{page2022,
  title = {Data and Code Availability Statements in Systematic Reviews of Interventions Were Often Missing or Inaccurate: A Content Analysis},
  shorttitle = {Data and Code Availability Statements in Systematic Reviews of Interventions Were Often Missing or Inaccurate},
  author = {Page, Matthew J. and Nguyen, Phi-Yen and Hamilton, Daniel G. and Haddaway, Neal R. and Kanukula, Raju and Moher, David and McKenzie, Joanne E.},
  year = {2022},
  month = jul,
  journal = {Journal of Clinical Epidemiology},
  volume = {147},
  pages = {1--10},
  issn = {0895-4356},
  doi = {10.1016/j.jclinepi.2022.03.003},
  abstract = {Objectives To estimate the frequency of data and code availability statements in a random sample of systematic reviews with meta-analysis of aggregate data, summarize the content of the statements and investigate how often data and code files were shared. Methods We searched for systematic reviews with meta-analysis of aggregate data on the effects of a health, social, behavioral, or educational intervention that were indexed in PubMed, Education Collection via ProQuest, Scopus via Elsevier, or Social Sciences Citation Index and Science Citation Index Expanded via Web of Science during a 4-week period (between November 2, and December 2, 2020). Records were randomly sorted and screened independently by two authors until our target sample of 300 systematic reviews was reached. Two authors independently recorded whether a data or code availability statement (or both) appeared in each review and coded the content of the statements using an inductive approach. Results Of the 300 included systematic reviews with meta-analysis, 86 (29\%) had a data availability statement, and seven (2\%) had both a data and code availability statement. In 12/93 (13\%) data availability statements, authors stated that data files were available for download from the journal website or a data repository, which we verified as being true. While 39/93 (42\%) authors stated data were available upon request, 37/93 (40\%) implied that sharing of data files was not necessary or applicable to them, most often because ``all data appear in the article'' or ``no datasets were generated or analyzed''. Discussion Data and code availability statements appear infrequently in systematic review manuscripts. Authors who do provide a data availability statement often incorrectly imply that data sharing is not applicable to systematic reviews. Our results suggest the need for various interventions to increase data and code sharing by systematic reviewers.},
  langid = {english},
  keywords = {Evidence synthesis,Open data,Open science,Open synthesis,Reproducibility of research,Research integrity},
  file = {C\:\\Users\\neeeb\\Zotero\\storage\\WZVMYMEI\\Page et al. - 2022 - Data and code availability statements in systemati.pdf;C\:\\Users\\neeeb\\Zotero\\storage\\AXFW86DS\\S0895435622000646.html}
}


@article{polanin2020,
  title = {Transparency and {{Reproducibility}} of {{Meta-Analyses}} in {{Psychology}}: {{A Meta-Review}}},
  shorttitle = {Transparency and {{Reproducibility}} of {{Meta-Analyses}} in {{Psychology}}},
  author = {Polanin, Joshua R. and Hennessy, Emily A. and Tsuji, Sho},
  year = {2020},
  month = jul,
  journal = {Perspectives on Psychological Science},
  volume = {15},
  number = {4},
  pages = {1026--1041},
  publisher = {{SAGE Publications Inc}},
  issn = {1745-6916},
  doi = {10.1177/1745691620906416},
  abstract = {Systematic review and meta-analysis are possible as viable research techniques only through transparent reporting of primary research; thus, one might expect meta-analysts to demonstrate best practice in their reporting of results and have a high degree of transparency leading to reproducibility of their work. This assumption has yet to be fully tested in the psychological sciences. We therefore aimed to assess the transparency and reproducibility of psychological meta-analyses. We conducted a meta-review by sampling 150 studies from Psychological Bulletin to extract information about each review's transparent and reproducible reporting practices. The results revealed that authors reported on average 55\% of criteria and that transparent reporting practices increased over the three decades studied (b = 1.09, SE = 0.24, t = 4.519, p {$<$} .001). Review authors consistently reported eligibility criteria, effect-size information, and synthesis techniques. Review authors, however, on average, did not report specific search results, screening and extraction procedures, and most importantly, effect-size and moderator information from each individual study. Far fewer studies provided statistical code required for complete analytical replication. We argue that the field of psychology and research synthesis in general should require review authors to report these elements in a transparent and reproducible manner.},
  langid = {english},
  keywords = {meta-analysis,reproducibility,research synthesis,research transparency}
}

  @Manual{r2022,
    title = {R: A Language and Environment for Statistical Computing},
    author = {{R Core Team}},
    organization = {R Foundation for Statistical Computing},
    address = {Vienna, Austria},
    year = {2022},
    url = {https://www.R-project.org/},
  }


@article{tedersoo2021,
  title = {Data Sharing Practices and Data Availability upon Request Differ across Scientific Disciplines},
  author = {Tedersoo, Leho and K{\"u}ngas, Rainer and Oras, Ester and K{\"o}ster, Kajar and Eenmaa, Helen and Leijen, {\"A}li and Pedaste, Margus and Raju, Marju and Astapova, Anastasiya and Lukner, Heli and Kogermann, Karin and Sepp, Tuul},
  year = {2021},
  month = jul,
  journal = {Scientific Data},
  volume = {8},
  number = {1},
  pages = {192},
  publisher = {{Nature Publishing Group}},
  issn = {2052-4463},
  doi = {10.1038/s41597-021-00981-0},
  abstract = {Data sharing is one of the cornerstones of modern science that enables large-scale analyses and reproducibility. We evaluated data availability in research articles across nine disciplines in Nature and Science magazines and recorded corresponding authors' concerns, requests and reasons for declining data sharing. Although data sharing has improved in the last decade and particularly in recent years, data availability and willingness to share data still differ greatly among disciplines. We observed that statements of data availability upon (reasonable) request are inefficient and should not be allowed by journals. To improve data sharing at the time of manuscript acceptance, researchers should be better motivated to release their data with real benefits such as recognition, or bonus points in grant and job applications. We recommend that data management costs should be covered by funding agencies; publicly available research data ought to be included in the evaluation of applications; and surveillance of data sharing should be enforced by both academic publishers and funders. These cross-discipline survey data are available from the plutoF repository.},
  copyright = {2021 The Author(s)},
  langid = {english},
  keywords = {Genetic databases,Molecular ecology},
  annotation = {Bandiera\_abtest: a Cc\_license\_type: cc\_by Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Genetic databases;Molecular ecology Subject\_term\_id: genetic-databases;molecular-ecology},
  file = {C\:\\Users\\neeeb\\Zotero\\storage\\J3ECTZ5V\\Tedersoo et al. - 2021 - Data sharing practices and data availability upon .pdf;C\:\\Users\\neeeb\\Zotero\\storage\\9HECQVG3\\s41597-021-00981-0.html}
}

@article{tendal2009,
  title = {Disagreements in Meta-Analyses Using Outcomes Measured on Continuous or Rating Scales: Observer Agreement Study},
  shorttitle = {Disagreements in Meta-Analyses Using Outcomes Measured on Continuous or Rating Scales},
  author = {Tendal, Britta and Higgins, Julian P. T. and J{\"u}ni, Peter and Hr{\'o}bjartsson, Asbj{\o}rn and Trelle, Sven and N{\"u}esch, Eveline and Wandel, Simon and J{\o}rgensen, Anders W. and Gesser, Katarina and {Ils{\o}e-Kristensen}, S{\o}ren and G{\o}tzsche, Peter C.},
  year = {2009},
  month = aug,
  journal = {BMJ},
  volume = {339},
  pages = {b3128},
  publisher = {{British Medical Journal Publishing Group}},
  issn = {0959-8138, 1468-5833},
  doi = {10.1136/bmj.b3128},
  abstract = {Objective To study the inter-observer variation related to extraction of continuous and numerical rating scale data from trial reports for use in meta-analyses. Design Observer agreement study. Data sources A random sample of 10 Cochrane reviews that presented a result as a standardised mean difference (SMD), the protocols for the reviews and the trial reports (n=45) were retrieved. Data extraction Five experienced methodologists and five PhD students independently extracted data from the trial reports for calculation of the first SMD result in each review. The observers did not have access to the reviews but to the protocols, where the relevant outcome was highlighted. The agreement was analysed at both trial and meta-analysis level, pairing the observers in all possible ways (45 pairs, yielding 2025 pairs of trials and 450 pairs of meta-analyses). Agreement was defined as SMDs that differed less than 0.1 in their point estimates or confidence intervals. Results The agreement was 53\% at trial level and 31\% at meta-analysis level. Including all pairs, the median disagreement was SMD=0.22 (interquartile range 0.07-0.61). The experts agreed somewhat more than the PhD students at trial level (61\% v 46\%), but not at meta-analysis level. Important reasons for disagreement were differences in selection of time points, scales, control groups, and type of calculations; whether to include a trial in the meta-analysis; and data extraction errors made by the observers. In 14 out of the 100 SMDs calculated at the meta-analysis level, individual observers reached different conclusions than the originally published review. Conclusions Disagreements were common and often larger than the effect of commonly used treatments. Meta-analyses using SMDs are prone to observer variation and should be interpreted with caution. The reliability of meta-analyses might be improved by having more detailed review protocols, more than one observer, and statistical expertise.},
  chapter = {Research},
  copyright = {\textcopyright{}  . This is an open-access article distributed under the terms of the Creative Commons Attribution Non-commercial License, which permits use, distribution, and reproduction in any medium, provided the original work is properly cited, the use is non commercial and is otherwise in compliance with the license. See: http://creativecommons.org/licenses/by-nc/2.0/  and  http://creativecommons.org/licenses/by-nc/2.0/legalcode.},
  langid = {english},
  pmid = {19679616},
  file = {C\:\\Users\\neeeb\\Zotero\\storage\\2RWC4V24\\Tendal et al. - 2009 - Disagreements in meta-analyses using outcomes meas.pdf}
}



@article{viechtbauer2010,
  title = {Conducting {{Meta-Analyses}} in {{R}} with the Metafor {{Package}}},
  author = {Viechtbauer, Wolfgang},
  year = {2010},
  month = aug,
  journal = {Journal of Statistical Software},
  volume = {36},
  pages = {1--48},
  issn = {1548-7660},
  doi = {10.18637/jss.v036.i03},
  abstract = {The metafor package provides functions for conducting meta-analyses in R. The package includes functions for fitting the meta-analytic fixed- and random-effects models and allows for the inclusion of moderators variables (study-level covariates) in these models. Meta-regression analyses with continuous and categorical moderators can be conducted in this way.  Functions for the Mantel-Haenszel and Peto's one-step method for meta-analyses of 2 x 2 table data are also available. Finally, the package provides various plot functions (for example, for forest, funnel, and radial plots) and functions for assessing the model fit, for obtaining case diagnostics, and for tests of publication bias.},
  copyright = {Copyright (c) 2009 Wolfgang Viechtbauer},
  langid = {english},
  file = {C\:\\Users\\neeeb\\Zotero\\storage\\MU3EDU6P\\Viechtbauer - 2010 - Conducting Meta-Analyses in R with the metafor Pac.pdf}
}

@article{wallach2018,
  title = {Reproducible Research Practices, Transparency, and Open Access Data in the Biomedical Literature, 2015\textendash 2017},
  author = {Wallach, Joshua D. and Boyack, Kevin W. and Ioannidis, John P. A.},
  year = {2018},
  month = nov,
  journal = {PLOS Biology},
  volume = {16},
  number = {11},
  pages = {e2006930},
  publisher = {{Public Library of Science}},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.2006930},
  abstract = {Currently, there is a growing interest in ensuring the transparency and reproducibility of the published scientific literature. According to a previous evaluation of 441 biomedical journals articles published in 2000\textendash 2014, the biomedical literature largely lacked transparency in important dimensions. Here, we surveyed a random sample of 149 biomedical articles published between 2015 and 2017 and determined the proportion reporting sources of public and/or private funding and conflicts of interests, sharing protocols and raw data, and undergoing rigorous independent replication and reproducibility checks. We also investigated what can be learned about reproducibility and transparency indicators from open access data provided on PubMed. The majority of the 149 studies disclosed some information regarding funding (103, 69.1\% [95\% confidence interval, 61.0\% to 76.3\%]) or conflicts of interest (97, 65.1\% [56.8\% to 72.6\%]). Among the 104 articles with empirical data in which protocols or data sharing would be pertinent, 19 (18.3\% [11.6\% to 27.3\%]) discussed publicly available data; only one (1.0\% [0.1\% to 6.0\%]) included a link to a full study protocol. Among the 97 articles in which replication in studies with different data would be pertinent, there were five replication efforts (5.2\% [1.9\% to 12.2\%]). Although clinical trial identification numbers and funding details were often provided on PubMed, only two of the articles without a full text article in PubMed Central that discussed publicly available data at the full text level also contained information related to data sharing on PubMed; none had a conflicts of interest statement on PubMed. Our evaluation suggests that although there have been improvements over the last few years in certain key indicators of reproducibility and transparency, opportunities exist to improve reproducible research practices across the biomedical literature and to make features related to reproducibility more readily visible in PubMed.},
  langid = {english},
  keywords = {Conflicts of interest,Government funding of science,Medical journals,Open science,Replication studies,Reproducibility,Scientific publishing,Systematic reviews},
  file = {C\:\\Users\\neeeb\\Zotero\\storage\\9AF7FZE9\\Wallach et al. - 2018 - Reproducible research practices, transparency, and.pdf;C\:\\Users\\neeeb\\Zotero\\storage\\PYXZMJ4G\\article.html}
}

@article{wilkinson2016,
  title = {The {{FAIR Guiding Principles}} for Scientific Data Management and Stewardship},
  author = {Wilkinson, Mark D. and Dumontier, Michel and Aalbersberg, IJsbrand Jan and Appleton, Gabrielle and Axton, Myles and Baak, Arie and Blomberg, Niklas and Boiten, Jan-Willem and {da Silva Santos}, Luiz Bonino and Bourne, Philip E. and Bouwman, Jildau and Brookes, Anthony J. and Clark, Tim and Crosas, Merc{\`e} and Dillo, Ingrid and Dumon, Olivier and Edmunds, Scott and Evelo, Chris T. and Finkers, Richard and {Gonzalez-Beltran}, Alejandra and Gray, Alasdair J. G. and Groth, Paul and Goble, Carole and Grethe, Jeffrey S. and Heringa, Jaap and {'t Hoen}, Peter A. C. and Hooft, Rob and Kuhn, Tobias and Kok, Ruben and Kok, Joost and Lusher, Scott J. and Martone, Maryann E. and Mons, Albert and Packer, Abel L. and Persson, Bengt and {Rocca-Serra}, Philippe and Roos, Marco and {van Schaik}, Rene and Sansone, Susanna-Assunta and Schultes, Erik and Sengstag, Thierry and Slater, Ted and Strawn, George and Swertz, Morris A. and Thompson, Mark and {van der Lei}, Johan and {van Mulligen}, Erik and Velterop, Jan and Waagmeester, Andra and Wittenburg, Peter and Wolstencroft, Katherine and Zhao, Jun and Mons, Barend},
  year = {2016},
  month = mar,
  journal = {Scientific Data},
  volume = {3},
  number = {1},
  pages = {160018},
  publisher = {{Nature Publishing Group}},
  issn = {2052-4463},
  doi = {10.1038/sdata.2016.18},
  abstract = {There is an urgent need to improve the infrastructure supporting the reuse of scholarly data. A diverse set of stakeholders\textemdash representing academia, industry, funding agencies, and scholarly publishers\textemdash have come together to design and jointly endorse a concise and measureable set of principles that we refer to as the FAIR Data Principles. The intent is that these may act as a guideline for those wishing to enhance the reusability of their data holdings. Distinct from peer initiatives that focus on the human scholar, the FAIR Principles put specific emphasis on enhancing the ability of machines to automatically find and use the data, in addition to supporting its reuse by individuals. This Comment is the first formal publication of the FAIR Principles, and includes the rationale behind them, and some exemplar implementations in the community.},
  copyright = {2016 The Author(s)},
  langid = {english},
  keywords = {Publication characteristics,Research data},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Comments \& Opinion Subject\_term: Publication characteristics;Research data Subject\_term\_id: publication-characteristics;research-data},
  file = {C\:\\Users\\neeeb\\Zotero\\storage\\8SBMLHAQ\\Wilkinson et al. - 2016 - The FAIR Guiding Principles for scientific data ma.pdf;C\:\\Users\\neeeb\\Zotero\\storage\\4JPG5ETU\\sdata201618.html}
}


@Manual{xie2022,
    title = {knitr: A General-Purpose Package for Dynamic Report
      Generation in R},
    author = {Yihui Xie},
    year = {2022},
    note = {R package version 1.39},
    url = {https://yihui.org/knitr/},
  }

