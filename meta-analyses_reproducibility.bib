  @Manual{allaire2022,
    title = {rmarkdown: Dynamic Documents for R},
    author = {JJ Allaire and Yihui Xie and Jonathan McPherson and
      Javier Luraschi and Kevin Ushey and Aron Atkins and Hadley
      Wickham and Joe Cheng and Winston Chang and Richard Iannone},
    year = {2022},
    note = {R package version 2.14},
    url = {https://github.com/rstudio/rmarkdown},
  }

@article{artner2020,
  title = {The Reproducibility of Statistical Results in Psychological Research: {{An}} Investigation Using Unpublished Raw Data},
  shorttitle = {The Reproducibility of Statistical Results in Psychological Research},
  author = {Artner, Richard and Verliefde, Thomas and Steegen, Sara and Gomes, Sara and Traets, Frits and Tuerlinckx, Francis and Vanpaemel, Wolf},
  year = {2020},
  journal = {Psychological Methods},
  pages = {No Pagination Specified-No Pagination Specified},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-1463},
  doi = {10.1037/met0000365},
  abstract = {We investigated the reproducibility of the major statistical conclusions drawn in 46 articles published in 2012 in three APA journals. After having identified 232 key statistical claims, we tried to reproduce, for each claim, the test statistic, its degrees of freedom, and the corresponding p value, starting from the raw data that were provided by the authors and closely following the Method section in the article. Out of the 232 claims, we were able to successfully reproduce 163 (70\%), 18 of which only by deviating from the article's analytical description. Thirteen (7\%) of the 185 claims deemed significant by the authors are no longer so. The reproduction successes were often the result of cumbersome and time-consuming trial-and-error work, suggesting that APA style reporting in conjunction with raw data makes numerical verification at least hard, if not impossible. This article discusses the types of mistakes we could identify and the tediousness of our reproduction efforts in the light of a newly developed taxonomy for reproducibility. We then link our findings with other findings of empirical research on this topic, give practical recommendations on how to achieve reproducibility, and discuss the challenges of large-scale reproducibility checks as well as promising ideas that could considerably increase the reproducibility of psychological research. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  keywords = {American Psychological Association,Data Sets,Errors,Experimental Replication,Experimentation,Scientific Communication,Statistical Analysis,Statistics}
}

  @Manual{aust2022,
    title = {{papaja}: {Prepare} reproducible {APA} journal articles with {R Markdown}},
    author = {Frederik Aust and Marius Barth},
    year = {2022},
    note = {R package version 0.1.1},
    url = {https://github.com/crsh/papaja},
  }


@article{bornmann2021,
  title = {Growth Rates of Modern Science: A Latent Piecewise Growth Curve Approach to Model Publication Numbers from Established and New Literature Databases},
  shorttitle = {Growth Rates of Modern Science},
  author = {Bornmann, Lutz and Haunschild, Robin and Mutz, R{\"u}diger},
  year = {2021},
  month = oct,
  journal = {Humanities and Social Sciences Communications},
  volume = {8},
  number = {1},
  pages = {1--15},
  publisher = {{Palgrave}},
  issn = {2662-9992},
  doi = {10.1057/s41599-021-00903-w},
  abstract = {Growth of science is a prevalent issue in science of science studies. In recent years, two new bibliographic databases have been introduced, which can be used to study growth processes in science from centuries back: Dimensions from Digital Science and Microsoft Academic. In this study, we used publication data from these new databases and added publication data from two established databases (Web of Science from Clarivate Analytics and Scopus from Elsevier) to investigate scientific growth processes from the beginning of the modern science system until today. We estimated regression models that included simultaneously the publication counts from the four databases. The results of the unrestricted growth of science calculations show that the overall growth rate amounts to 4.10\% with a doubling time of 17.3 years. As the comparison of various segmented regression models in the current study revealed, models with four or five segments fit the publication data best. We demonstrated that these segments with different growth rates can be interpreted very well, since they are related to either phases of economic (e.g., industrialization) and/or political developments (e.g., Second World War). In this study, we additionally analyzed scientific growth in two broad fields (Physical and Technical Sciences as well as Life Sciences) and the relationship of scientific and economic growth in UK. The comparison between the two fields revealed only slight differences. The comparison of the British economic and scientific growth rates showed that the economic growth rate is slightly lower than the scientific growth rate.},
  copyright = {2021 The Author(s)},
  langid = {english},
  keywords = {Science,Sociology,technology and society},
  annotation = {Bandiera\_abtest: a Cc\_license\_type: cc\_by Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Science, technology and society;Sociology Subject\_term\_id: science-technology-and-society;sociology},
  file = {C\:\\Users\\neeeb\\Zotero\\storage\\Z8AXIFKF\\Bornmann et al. - 2021 - Growth rates of modern science a latent piecewise.pdf;C\:\\Users\\neeeb\\Zotero\\storage\\78A9YB82\\s41599-021-00903-w.html}
}


@article{errington2021a,
  title = {Investigating the Replicability of Preclinical Cancer Biology},
  author = {Errington, Timothy M and Mathur, Maya and Soderberg, Courtney K and Denis, Alexandria and Perfito, Nicole and Iorns, Elizabeth and Nosek, Brian A},
  editor = {Pasqualini, Renata and Franco, Eduardo},
  year = {2021},
  month = dec,
  journal = {eLife},
  volume = {10},
  pages = {e71601},
  publisher = {{eLife Sciences Publications, Ltd}},
  issn = {2050-084X},
  doi = {10.7554/eLife.71601},
  abstract = {Replicability is an important feature of scientific research, but aspects of contemporary research culture, such as an emphasis on novelty, can make replicability seem less important than it should be. The Reproducibility Project: Cancer Biology was set up to provide evidence about the replicability of preclinical research in cancer biology by repeating selected experiments from high-impact papers. A total of 50 experiments from 23 papers were repeated, generating data about the replicability of a total of 158 effects. Most of the original effects were positive effects (136), with the rest being null effects (22). A majority of the original effect sizes were reported as numerical values (117), with the rest being reported as representative images (41). We employed seven methods to assess replicability, and some of these methods were not suitable for all the effects in our sample. One method compared effect sizes: for positive effects, the median effect size in the replications was 85\% smaller than the median effect size in the original experiments, and 92\% of replication effect sizes were smaller than the original. The other methods were binary \textendash{} the replication was either a success or a failure \textendash{} and five of these methods could be used to assess both positive and null effects when effect sizes were reported as numerical values. For positive effects, 40\% of replications (39/97) succeeded according to three or more of these five methods, and for null effects 80\% of replications (12/15) were successful on this basis; combining positive and null effects, the success rate was 46\% (51/112). A successful replication does not definitively confirm an original finding or its theoretical interpretation. Equally, a failure to replicate does not disconfirm a finding, but it does suggest that additional investigation is needed to establish its reliability.},
  keywords = {credibility,meta-analysis,replication,reproducibility,reproducibility in cancer biology,Reproducibility Project: Cancer Biology,transparency},
  file = {C\:\\Users\\neeeb\\Zotero\\storage\\SP7UWH9S\\Errington et al. - 2021 - Investigating the replicability of preclinical can.pdf}
}

@article{gabelica2022,
  title = {Many Researchers Were Not Compliant with Their Published Data Sharing Statement: Mixed-Methods Study},
  shorttitle = {Many Researchers Were Not Compliant with Their Published Data Sharing Statement},
  author = {Gabelica, Mirko and Boj{\v c}i{\'c}, Ru{\v z}ica and Puljak, Livia},
  year = {2022},
  month = may,
  journal = {Journal of Clinical Epidemiology},
  issn = {0895-4356},
  doi = {10.1016/j.jclinepi.2022.05.019},
  abstract = {Objectives To analyse researchers' compliance with their Data Availability Statement (DAS) from manuscripts published in open access journals with the mandatory DAS. Study Design and Setting We analyzed all articles from 333 open-access journals published during January 2019 by BioMed Central. We categorized types of DAS. We surveyed corresponding authors who wrote in DAS that they would share the data. A consent to participate in the study was sought for all included manuscripts. After accessing raw data sets, we checked whether data were available in a way that enabled re-analysis. Results Of 3556 analyzed articles, 3416 contained DAS. The most frequent DAS category (42\%) indicated that the datasets are available on reasonable request. Among 1792 manuscripts in which DAS indicated that authors are willing to share their data, 1670 (93\%) authors either did not respond or declined to share their data with us. Among 254 (14\%) of 1792 authors who responded to our query for data sharing, only 122 (6.8\%) provided the requested data. Conclusion Even when authors indicate in their manuscript that they will share data upon request, the compliance rate is the same as for authors who do not provide DAS, suggesting that DAS may not be sufficient to ensure data sharing.},
  langid = {english},
  keywords = {Data availability statement,data sharing,non-compliance,open data},
  file = {C\:\\Users\\neeeb\\Zotero\\storage\\UA53VFJX\\Gabelica et al. - 2022 - Many researchers were not compliant with their pub.pdf;C\:\\Users\\neeeb\\Zotero\\storage\\XHYBCAK6\\S089543562200141X.html}
}


@article{gotzsche2007,
  title = {Data {{Extraction Errors}} in {{Meta-analyses That Use Standardized Mean Differences}}},
  author = {G{\o}tzsche, Peter C. and Hr{\'o}bjartsson, Asbj{\o}rn and Mari{\'c}, Katja and Tendal, Britta},
  year = {2007},
  month = jul,
  journal = {JAMA},
  volume = {298},
  number = {4},
  pages = {430--437},
  issn = {0098-7484},
  doi = {10.1001/jama.298.4.430},
  abstract = {ContextMeta-analysis of trials that have used different continuous or rating scales to record outcomes of a similar nature requires sophisticated data handling and data transformation to a uniform scale, the standardized mean difference (SMD). It is not known how reliable such meta-analyses are.ObjectiveTo study whether SMDs in meta-analyses are accurate.Data SourcesSystematic review of meta-analyses published in 2004 that reported a result as an SMD, with no language restrictions. Two trials were randomly selected from each meta-analysis. We attempted to replicate the results in each meta-analysis by independently calculating SMD using Hedges adjusted g.Data ExtractionOur primary outcome was the proportion of meta-analyses for which our result differed from that of the authors by 0.1 or more, either for the point estimate or for its confidence interval, for at least 1 of the 2 selected trials. We chose 0.1 as cut point because many commonly used treatments have an effect of 0.1 to 0.5, compared with placebo.ResultsOf the 27 meta-analyses included in this study, we could not replicate the result for at least 1 of the 2 trials within 0.1 in 10 of the meta-analyses (37\%), and in 4 cases, the discrepancy was 0.6 or more for the point estimate. Common problems were erroneous number of patients, means, standard deviations, and sign for the effect estimate. In total, 17 meta-analyses (63\%) had errors for at least 1 of the 2 trials examined. For the 10 meta-analyses with errors of at least 0.1, we checked the data from all the trials and conducted our own meta-analysis, using the authors' methods. Seven of these 10 meta-analyses were erroneous (70\%); 1 was subsequently retracted, and in 2 a significant difference disappeared or appeared.ConclusionsThe high proportion of meta-analyses based on SMDs that show errors indicates that although the statistical process is ostensibly simple, data extraction is particularly liable to errors that can negate or even reverse the findings of the study. This has implications for researchers and implies that all readers, including journal reviewers and policy makers, should approach such meta-analyses with caution.}
}


@article{gurevitch2018,
  title = {Meta-Analysis and the Science of Research Synthesis},
  author = {Gurevitch, Jessica and Koricheva, Julia and Nakagawa, Shinichi and Stewart, Gavin},
  year = {2018},
  month = mar,
  journal = {Nature},
  volume = {555},
  number = {7695},
  pages = {175--182},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/nature25753},
  abstract = {Meta-analysis is the quantitative, scientific synthesis of research results. Since the term and modern approaches to research synthesis were first introduced in the 1970s, meta-analysis has had a revolutionary effect in many scientific fields, helping to establish evidence-based practice and to resolve seemingly contradictory research outcomes. At the same time, its implementation has engendered criticism and controversy, in some cases general and others specific to particular disciplines. Here we take the opportunity provided by the recent fortieth anniversary of meta-analysis to reflect on the accomplishments, limitations, recent advances and directions for future developments in the field of research synthesis.},
  copyright = {2018 Macmillan Publishers Limited, part of Springer Nature. All rights reserved.},
  langid = {english},
  keywords = {Biodiversity,Outcomes research},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Reviews Subject\_term: Biodiversity;Outcomes research Subject\_term\_id: biodiversity;outcomes-research},
  file = {C\:\\Users\\neeeb\\Zotero\\storage\\RMPXSGX2\\nature25753.html}
}

@article{hardwicke2018,
  title = {Data Availability, Reusability, and Analytic Reproducibility: Evaluating the Impact of a Mandatory Open Data Policy at the Journal {{Cognition}}},
  shorttitle = {Data Availability, Reusability, and Analytic Reproducibility},
  author = {Hardwicke, Tom E. and Mathur, Maya B. and MacDonald, Kyle and Nilsonne, Gustav and Banks, George C. and Kidwell, Mallory C. and Hofelich Mohr, Alicia and Clayton, Elizabeth and Yoon, Erica J. and Henry Tessler, Michael and Lenne, Richie L. and Altman, Sara and Long, Bria and Frank, Michael C.},
  year = {2018},
  month = aug,
  journal = {Royal Society Open Science},
  volume = {5},
  number = {8},
  pages = {180448},
  issn = {2054-5703},
  doi = {10.1098/rsos.180448},
  abstract = {Access to data is a critical feature of an efficient, progressive and ultimately self-correcting scientific ecosystem. But the extent to which in-principle benefits of data sharing are realized in practice is unclear. Crucially, it is largely unknown whether published findings can be reproduced by repeating reported analyses upon shared data ('analytic reproducibility'). To investigate this, we conducted an observational evaluation of a mandatory open data policy introduced at the journal Cognition. Interrupted time-series analyses indicated a substantial post-policy increase in data available statements (104/417, 25\% pre-policy to 136/174, 78\% post-policy), although not all data appeared reusable (23/104, 22\% pre-policy to 85/136, 62\%, post-policy). For 35 of the articles determined to have reusable data, we attempted to reproduce 1324 target values. Ultimately, 64 values could not be reproduced within a 10\% margin of error. For 22 articles all target values were reproduced, but 11 of these required author assistance. For 13 articles at least one value could not be reproduced despite author assistance. Importantly, there were no clear indications that original conclusions were seriously impacted. Mandatory open data policies can increase the frequency and quality of data sharing. However, suboptimal data curation, unclear analysis specification and reporting errors can impede analytic reproducibility, undermining the utility of data sharing and the credibility of scientific findings.},
  langid = {english},
  pmcid = {PMC6124055},
  pmid = {30225032},
  keywords = {interrupted time series,journal policy,meta-science,open data,open science,reproducibility},
  file = {C\:\\Users\\neeeb\\Zotero\\storage\\VSJVKCZL\\Hardwicke et al. - 2018 - Data availability, reusability, and analytic repro.pdf}
}

@article{hardwicke2020,
  title = {An Empirical Assessment of Transparency and Reproducibility-Related Research Practices in the Social Sciences (2014-2017)},
  author = {Hardwicke, Tom E. and Wallach, Joshua D. and Kidwell, Mallory C. and Bendixen, Theiss and Cr{\"u}well, Sophia and Ioannidis, John P. A.},
  year = {2020},
  month = feb,
  journal = {Royal Society Open Science},
  volume = {7},
  number = {2},
  pages = {190806},
  issn = {2054-5703},
  doi = {10.1098/rsos.190806},
  abstract = {Serious concerns about research quality have catalysed a number of reform initiatives intended to improve transparency and reproducibility and thus facilitate self-correction, increase efficiency and enhance research credibility. Meta-research has evaluated the merits of some individual initiatives; however, this may not capture broader trends reflecting the cumulative contribution of these efforts. In this study, we manually examined a random sample of 250 articles in order to estimate the prevalence of a range of transparency and reproducibility-related indicators in the social sciences literature published between 2014 and 2017. Few articles indicated availability of materials (16/151, 11\% [95\% confidence interval, 7\% to 16\%]), protocols (0/156, 0\% [0\% to 1\%]), raw data (11/156, 7\% [2\% to 13\%]) or analysis scripts (2/156, 1\% [0\% to 3\%]), and no studies were pre-registered (0/156, 0\% [0\% to 1\%]). Some articles explicitly disclosed funding sources (or lack of; 74/236, 31\% [25\% to 37\%]) and some declared no conflicts of interest (36/236, 15\% [11\% to 20\%]). Replication studies were rare (2/156, 1\% [0\% to 3\%]). Few studies were included in evidence synthesis via systematic review (17/151, 11\% [7\% to 16\%]) or meta-analysis (2/151, 1\% [0\% to 3\%]). Less than half the articles were publicly available (101/250, 40\% [34\% to 47\%]). Minimal adoption of transparency and reproducibility-related research practices could be undermining the credibility and efficiency of social science research. The present study establishes a baseline that can be revisited in the future to assess progress.},
  langid = {english},
  pmcid = {PMC7062098},
  pmid = {32257301},
  keywords = {meta-research,open science,reproducibility,social sciences,transparency},
  file = {C\:\\Users\\neeeb\\Zotero\\storage\\6VTTUL42\\Hardwicke et al. - 2020 - An empirical assessment of transparency and reprod.pdf}
}

@article{hardwicke2021,
  title = {Analytic Reproducibility in Articles Receiving Open Data Badges at the Journal {{Psychological Science}}: An Observational Study},
  shorttitle = {Analytic Reproducibility in Articles Receiving Open Data Badges at the Journal {{Psychological Science}}},
  author = {Hardwicke, Tom E. and Bohn, Manuel and MacDonald, Kyle and Hembacher, Emily and Nuijten, Mich{\`e}le B. and Peloquin, Benjamin N. and {deMayo}, Benjamin E. and Long, Bria and Yoon, Erica J. and Frank, Michael C.},
  year = {2021},
  month = jan,
  journal = {Royal Society Open Science},
  volume = {8},
  number = {1},
  pages = {201494},
  issn = {2054-5703},
  doi = {10.1098/rsos.201494},
  abstract = {For any scientific report, repeating the original analyses upon the original data should yield the original outcomes. We evaluated analytic reproducibility in 25 Psychological Science articles awarded open data badges between 2014 and 2015. Initially, 16 (64\%, 95\% confidence interval [43,81]) articles contained at least one 'major numerical discrepancy' ({$>$}10\% difference) prompting us to request input from original authors. Ultimately, target values were reproducible without author involvement for 9 (36\% [20,59]) articles; reproducible with author involvement for 6 (24\% [8,47]) articles; not fully reproducible with no substantive author response for 3 (12\% [0,35]) articles; and not fully reproducible despite author involvement for 7 (28\% [12,51]) articles. Overall, 37 major numerical discrepancies remained out of 789 checked values (5\% [3,6]), but original conclusions did not appear affected. Non-reproducibility was primarily caused by unclear reporting of analytic procedures. These results highlight that open data alone is not sufficient to ensure analytic reproducibility.},
  langid = {english},
  pmcid = {PMC7890505},
  pmid = {33614084},
  keywords = {journal policy,meta-research,open badges,open data,open science,reproducibility},
  file = {C\:\\Users\\neeeb\\Zotero\\storage\\FTK9RTE8\\Hardwicke et al. - 2021 - Analytic reproducibility in articles receiving ope.pdf}
}

@article{hardwicke2022,
  title = {Estimating the {{Prevalence}} of {{Transparency}} and {{Reproducibility-Related Research Practices}} in {{Psychology}} (2014\textendash 2017)},
  author = {Hardwicke, Tom E. and Thibault, Robert T. and Kosie, Jessica E. and Wallach, Joshua D. and Kidwell, Mallory C. and Ioannidis, John P. A.},
  year = {2022},
  month = jan,
  journal = {Perspectives on Psychological Science},
  volume = {17},
  number = {1},
  pages = {239--251},
  publisher = {{SAGE Publications Inc}},
  issn = {1745-6916},
  doi = {10.1177/1745691620979806},
  abstract = {Psychologists are navigating an unprecedented period of introspection about the credibility and utility of their discipline. Reform initiatives emphasize the benefits of transparency and reproducibility-related research practices; however, adoption across the psychology literature is unknown. Estimating the prevalence of such practices will help to gauge the collective impact of reform initiatives, track progress over time, and calibrate future efforts. To this end, we manually examined a random sample of 250 psychology articles published between 2014 and 2017. Over half of the articles were publicly available (154/237, 65\%, 95\% confidence interval [CI] = [59\%, 71\%]); however, sharing of research materials (26/183; 14\%, 95\% CI = [10\%, 19\%]), study protocols (0/188; 0\%, 95\% CI = [0\%, 1\%]), raw data (4/188; 2\%, 95\% CI = [1\%, 4\%]), and analysis scripts (1/188; 1\%, 95\% CI = [0\%, 1\%]) was rare. Preregistration was also uncommon (5/188; 3\%, 95\% CI = [1\%, 5\%]). Many articles included a funding disclosure statement (142/228; 62\%, 95\% CI = [56\%, 69\%]), but conflict-of-interest statements were less common (88/228; 39\%, 95\% CI = [32\%, 45\%]). Replication studies were rare (10/188; 5\%, 95\% CI = [3\%, 8\%]), and few studies were included in systematic reviews (21/183; 11\%, 95\% CI = [8\%, 16\%]) or meta-analyses (12/183; 7\%, 95\% CI = [4\%, 10\%]). Overall, the results suggest that transparency and reproducibility-related research practices were far from routine. These findings establish baseline prevalence estimates against which future progress toward increasing the credibility and utility of psychology research can be compared.},
  langid = {english},
  keywords = {meta-research,open science,psychology,reproducibility,transparency},
  file = {C\:\\Users\\neeeb\\Zotero\\storage\\CBCM8ART\\Hardwicke et al. - 2022 - Estimating the Prevalence of Transparency and Repr.pdf}
}


@article{klein2014a,
  title = {Investigating {{Variation}} in {{Replicability}}},
  author = {Klein, Richard A. and Ratliff, Kate A. and Vianello, Michelangelo and Adams, Reginald B. and Bahn{\'i}k, {\v S}t{\v e}p{\'a}n and Bernstein, Michael J. and Bocian, Konrad and Brandt, Mark J. and Brooks, Beach and Brumbaugh, Claudia Chloe and Cemalcilar, Zeynep and Chandler, Jesse and Cheong, Winnee and Davis, William E. and Devos, Thierry and Eisner, Matthew and Frankowska, Natalia and Furrow, David and Galliani, Elisa Maria and Hasselman, Fred and Hicks, Joshua A. and Hovermale, James F. and Hunt, S. Jane and Huntsinger, Jeffrey R. and IJzerman, Hans and John, Melissa-Sue and {Joy-Gaba}, Jennifer A. and Barry Kappes, Heather and Krueger, Lacy E. and Kurtz, Jaime and Levitan, Carmel A. and Mallett, Robyn K. and Morris, Wendy L. and Nelson, Anthony J. and Nier, Jason A. and Packard, Grant and Pilati, Ronaldo and Rutchick, Abraham M. and Schmidt, Kathleen and Skorinko, Jeanine L. and Smith, Robert and Steiner, Troy G. and Storbeck, Justin and Van Swol, Lyn M. and Thompson, Donna and {van `t Veer}, A. E. and Ann Vaughn, Leigh and Vranka, Marek and Wichman, Aaron L. and Woodzicka, Julie A. and Nosek, Brian A.},
  year = {2014},
  month = may,
  journal = {Social Psychology},
  volume = {45},
  number = {3},
  pages = {142--152},
  publisher = {{Hogrefe Publishing}},
  issn = {1864-9335},
  doi = {10.1027/1864-9335/a000178},
  abstract = {Although replication is a central tenet of science, direct replications are rare in psychology. This research tested variation in the replicability of 13 classic and contemporary effects across 36 independent samples totaling 6,344 participants. In the aggregate, 10 effects replicated consistently. One effect \textendash{} imagined contact reducing prejudice \textendash{} showed weak support for replicability. And two effects \textendash{} flag priming influencing conservatism and currency priming influencing system justification \textendash{} did not replicate. We compared whether the conditions such as lab versus online or US versus international sample predicted effect magnitudes. By and large they did not. The results of this small sample of effects suggest that replicability is more dependent on the effect itself than on the sample and setting used to investigate the effect.},
  keywords = {cross-cultural,generalizability,replication,reproducibility,variation},
  file = {C\:\\Users\\neeeb\\Zotero\\storage\\MCMSQHPD\\Klein et al. - 2014 - Investigating Variation in Replicability.pdf}
}

@article{koffel016,
	title = {Reproducibility of {Search} {Strategies} {Is} {Poor} in {Systematic} {Reviews} {Published} in {High}-{Impact} {Pediatrics}, {Cardiology} and {Surgery} {Journals}: {A} {Cross}-{Sectional} {Study}},
	volume = {11},
	issn = {1932-6203},
	shorttitle = {Reproducibility of {Search} {Strategies} {Is} {Poor} in {Systematic} {Reviews} {Published} in {High}-{Impact} {Pediatrics}, {Cardiology} and {Surgery} {Journals}},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0163309},
	doi = {10.1371/journal.pone.0163309},
	abstract = {Background A high-quality search strategy is considered an essential component of systematic reviews but many do not contain reproducible search strategies. It is unclear if low reproducibility spans medical disciplines, is affected by librarian/search specialist involvement or has improved with increased awareness of reporting guidelines. Objectives To examine the reporting of search strategies in systematic reviews published in Pediatrics, Surgery or Cardiology journals in 2012 and determine rates and predictors of including a reproducible search strategy. Methods We identified all systematic reviews published in 2012 in the ten highest impact factor journals in Pediatrics, Surgery and Cardiology. Each search strategy was coded to indicate what elements were reported and whether the overall search was reproducible. Reporting and reproducibility rates were compared across disciplines and we measured the influence of librarian/search specialist involvement, discipline or endorsement of a reporting guideline on search reproducibility. Results 272 articles from 25 journals were included. Reporting of search elements ranged widely from 91\% of articles naming search terms to 33\% providing a full search strategy and 22\% indicating the date the search was executed. Only 22\% of articles provided at least one reproducible search strategy and 13\% provided a reproducible strategy for all databases searched in the article. Librarians or search specialists were reported as involved in 17\% of articles. There were strong disciplinary differences on the reporting of search elements. In the multivariable analysis, only discipline (Pediatrics) was a significant predictor of the inclusion of a reproducible search strategy. Conclusions Despite recommendations to report full, reproducible search strategies, many articles still do not. In addition, authors often report a single strategy as covering all databases searched, further decreasing reproducibility. Further research is needed to determine how disciplinary culture may encourage reproducibility and the role that journal editors and peer reviewers could play.},
	language = {en},
	number = {9},
	urldate = {2022-11-07},
	journal = {PLOS ONE},
	author = {Koffel, Jonathan B. and Rethlefsen, Melissa L.},
	month = sep,
	year = {2016},
	note = {Publisher: Public Library of Science},
	keywords = {Database searching, Medical journals, Pediatric cardiology, Pediatric surgery, Pediatrics, Reproducibility, Research reporting guidelines, Systematic reviews},
	pages = {e0163309},
	file = {Full Text PDF:C\:\\Users\\neeeb\\Zotero\\storage\\KVQSUXE2\\Koffel y Rethlefsen - 2016 - Reproducibility of Search Strategies Is Poor in Sy.pdf:application/pdf},
}



@article{lopez-nicolas2021,
	title = {A meta-review of transparency and reproducibility-related reporting practices in published meta-analyses on clinical psychological interventions (2000–2020)},
	volume = {54},
	issn = {1554-3528},
	url = {https://doi.org/10.3758/s13428-021-01644-z},
	doi = {10.3758/s13428-021-01644-z},
	abstract = {Meta-analysis is a powerful and important tool to synthesize the literature about a research topic. Like other kinds of research, meta-analyses must be reproducible to be compliant with the principles of the scientific method. Furthermore, reproducible meta-analyses can be easily updated with new data and reanalysed applying new and more refined analysis techniques. We attempted to empirically assess the prevalence of transparency and reproducibility-related reporting practices in published meta-analyses from clinical psychology by examining a random sample of 100 meta-analyses. Our purpose was to identify the key points that could be improved, with the aim of providing some recommendations for carrying out reproducible meta-analyses. We conducted a meta-review of meta-analyses of psychological interventions published between 2000 and 2020. We searched PubMed, PsycInfo and Web of Science databases. A structured coding form to assess transparency indicators was created based on previous studies and existing meta-analysis guidelines. We found major issues concerning: completely reproducible search procedures report, specification of the exact method to compute effect sizes, choice of weighting factors and estimators, lack of availability of the raw statistics used to compute the effect size and of interoperability of available data, and practically total absence of analysis script code sharing. Based on our findings, we conclude with recommendations intended to improve the transparency, openness, and reproducibility-related reporting practices of meta-analyses in clinical psychology and related areas.},
	language = {en},
	number = {1},
	urldate = {2022-11-07},
	journal = {Behavior Research Methods},
	author = {López-Nicolás, Rubén and López-López, José Antonio and Rubio-Aparicio, María and Sánchez-Meca, Julio},
	month = feb,
	year = {2022},
	keywords = {Data sharing, Meta-analysis, Meta-science, Reproducibility, Transparency and openness practices},
	pages = {334--349},
	file = {Full Text PDF:C\:\\Users\\neeeb\\Zotero\\storage\\SHIB8ZTH\\López-Nicolás et al. - 2022 - A meta-review of transparency and reproducibility-.pdf:application/pdf},
}

@article{maggio2011,
	title = {Reproducibility of {Literature} {Search} {Reporting} in {Medical} {Education} {Reviews}},
	volume = {86},
	issn = {1040-2446},
	url = {https://journals.lww.com/academicmedicine/Fulltext/2011/08000/Reproducibility_of_Literature_Search_Reporting_in.30.aspx},
	doi = {10.1097/ACM.0b013e31822221e7},
	abstract = {Purpose 
        Medical education literature has been found to lack key components of scientific reporting, including adequate descriptions of literature searches, thus preventing medical educators from replicating and building on previous scholarship. The purpose of this study was to examine the reproducibility of search strategies as reported in medical education literature reviews.
        Method 
        The authors searched for and identified literature reviews published in 2009 in Academic Medicine, Teaching and Learning in Medicine, and Medical Education. They searched for citations whose titles included the words “meta-analysis,” “systematic literature review,” “systematic review,” or “literature review,” or whose publication type MEDLINE listed as “meta-analysis” or “review.” The authors created a checklist to identify key characteristics of literature searches and of literature search reporting within the full text of the reviews. The authors deemed searches reproducible only if the review reported both a search date and Boolean operators.
        Results 
        Of the 34 reviews meeting the inclusion criteria, 19 (56\%) explicitly described a literature search and mentioned MEDLINE; however, only 14 (41\%) also mentioned searches of nonmedical databases. Eighteen reviews (53\%) listed search terms, but only 6 (18\%) listed Medical Subject Headings, and only 2 (6\%) mentioned Boolean operators. Fifteen (44\%) noted the use of limits. None of the reviews included reproducible searches.
        Conclusions 
        According to this analysis, literature search strategies in medical education reviews are highly variable and generally not reproducible. The authors provide recommendations to facilitate future high-quality, transparent, and reproducible literature searches.},
	language = {en-US},
	number = {8},
	urldate = {2022-11-07},
	journal = {Academic Medicine},
	author = {Maggio, Lauren A. and Tannery, Nancy H. and Kanter, Steven L.},
	month = aug,
	year = {2011},
	pages = {1049--1054},
	file = {Snapshot:C\:\\Users\\neeeb\\Zotero\\storage\\M9PWSPWW\\Reproducibility_of_Literature_Search_Reporting_in.30.html:text/html},
}


@article{maassen2020,
  title = {Reproducibility of Individual Effect Sizes in Meta-Analyses in Psychology},
  author = {Maassen, Esther and van Assen, Marcel A. L. M. and Nuijten, Mich{\`e}le B. and {Olsson-Collentine}, Anton and Wicherts, Jelte M.},
  year = {2020},
  month = may,
  journal = {PLOS ONE},
  volume = {15},
  number = {5},
  pages = {e0233107},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0233107},
  abstract = {To determine the reproducibility of psychological meta-analyses, we investigated whether we could reproduce 500 primary study effect sizes drawn from 33 published meta-analyses based on the information given in the meta-analyses, and whether recomputations of primary study effect sizes altered the overall results of the meta-analysis. Results showed that almost half (k = 224) of all sampled primary effect sizes could not be reproduced based on the reported information in the meta-analysis, mostly because of incomplete or missing information on how effect sizes from primary studies were selected and computed. Overall, this led to small discrepancies in the computation of mean effect sizes, confidence intervals and heterogeneity estimates in 13 out of 33 meta-analyses. We provide recommendations to improve transparency in the reporting of the entire meta-analytic process, including the use of preregistration, data and workflow sharing, and explicit coding practices.},
  langid = {english},
  keywords = {Clinical psychology,Metaanalysis,Peer review,Psychology,Publication ethics,Reproducibility,Research reporting guidelines,Systematic reviews},
  file = {C\:\\Users\\neeeb\\Zotero\\storage\\ZEHFMN8P\\Maassen et al. - 2020 - Reproducibility of individual effect sizes in meta.pdf}
}

@article{moreau2020,
  title={Conducting a meta-analysis in the age of open science: Tools, tips, and practical recommendations.},
  author={Moreau, David and Gamble, Beau},
  journal={Psychological Methods},
  volume={27},
  number={3},
  pages={426-432},
  doi={10.1037/met0000351},
  year={2020},
  publisher={American Psychological Association}
}


@book{nationalacademiesofsciencesengineeringandmedicine2019,
  title = {Reproducibility and {{Replicability}} in {{Science}}},
  author = {{National Academies of Sciences, Engineering, and Medicine}},
  year = {2019},
  publisher = {{The National Academies Press}},
  address = {{Washington, DC}},
  doi = {10.17226/25303},
  abstract = {One of the pathways by which the scientific community confirms the validity of a new scientific discovery is by repeating the research that produced it. When a scientific effort fails to independently confirm the computations or results of a previous study, some fear that it may be a symptom of a lack of rigor in science, while others argue that such an observed inconsistency can be an important precursor to new discovery. Concerns about reproducibility and replicability have been expressed in both scientific and popular media. As these concerns came to light, Congress requested that the National Academies of Sciences, Engineering, and Medicine conduct a study to assess the extent of issues related to reproducibility and replicability and to offer recommendations for improving rigor and transparency in scientific research. Reproducibility and Replicability in Science defines reproducibility and replicability and examines the factors that may lead to non-reproducibility and non-replicability in research. Unlike the typical expectation of reproducibility between two computations, expectations about replicability are more nuanced, and in some cases a lack of replicability can aid the process of scientific discovery. This report provides recommendations to researchers, academic institutions, journals, and funders on steps they can take to improve reproducibility and replicability in science.},
  isbn = {978-0-309-48616-3},
  langid = {english},
  keywords = {Policy for Science and Technology,Surveys and Statistics}
}

@misc{nguyen2022,
	title = {Changing patterns in reporting and sharing of review data in systematic reviews with meta-analysis of the effects of interventions: a meta-research study},
	copyright = {© 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	shorttitle = {Changing patterns in reporting and sharing of review data in systematic reviews with meta-analysis of the effects of interventions},
	url = {https://www.medrxiv.org/content/10.1101/2022.04.11.22273688v3},
	doi = {10.1101/2022.04.11.22273688},
	abstract = {Objectives To examine changes in completeness of reporting and frequency of sharing data, analytic code and other review materials in systematic reviews (SRs) over time; and factors associated with these changes.
Design Cross-sectional meta-research study.
Sample A random sample of 300 SRs with meta-analysis of aggregate data on the effects of a health, social, behavioural or educational intervention, which were indexed in PubMed, Science Citation Index, Social Sciences Citation Index, Scopus and Education Collection in November 2020.
Analysis/Outcomes The extent of complete reporting and frequency of sharing review materials in these reviews were compared with 110 SRs indexed in February 2014. Associations between completeness of reporting and various factors (e.g. self-reported use of reporting guidelines, journal’s data sharing policies) were examined by calculating risk ratios (RR) and 95\% confidence intervals (CI).
Results Several items were reported sub-optimally among 300 SRs from 2020, such as a registration record for the review (38\%), a full search strategy for at least one database (71\%), methods used to assess risk of bias (62\%), methods used to prepare data for meta-analysis (34\%), and funding source for the review (72\%). Only a few items not already reported at a high frequency in 2014 were reported more frequently in 2020. There was no evidence that reviews using a reporting guideline were more completely reported than reviews not using a guideline. Reviews published in 2020 in journals that mandated either data sharing or inclusion of Data Availability Statements were more likely to share their review materials (e.g. data, code files) (18\% vs 2\%).
Conclusion Incomplete reporting of several recommended items for systematic reviews persists, even in reviews that claim to have followed a reporting guideline. Data sharing policies of journals potentially encourage sharing of review materials.
SUMMARY BOX What is already known on this topicWhat is already known on this topicComplete reporting of methods and results, as well as sharing data and analytic code, enhances transparency and reproducibility of systematic reviews. The extent of complete reporting and sharing of data or analytic code among systematic reviews needs to be comprehensively assessed.Use of reporting guidelines, which are designed to improve reporting in systematic reviews, is increasing. It is unclear whether this increase has had an impact on reporting of methods and results in systematic reviews.More journals are adopting open data policies which aim to promote data sharing. The impact of these policies on sharing data and analytic code in systematic reviews is also unclear.What this study addsWhat this study addsIncomplete reporting of several recommended items in systematic reviews persists. Frequency of sharing review data and analytic code is currently low (7\%).An increase in self-reported use of a reporting guideline was observed between 2014-2020; however, there was no evidence that reviews using a reporting guideline were more completely reported than reviews not using a guideline.Reviews published in 2020 in journals that mandated either data sharing or inclusion of Data Availability Statements were more likely to share their review materials (e.g. data, code files).},
	language = {en},
	urldate = {2022-11-07},
	publisher = {medRxiv},
	author = {Nguyen, Phi-Yen and Kanukula, Raju and McKenzie, Joanne E. and Alqaidoom, Zainab and Brennan, Sue E. and Haddaway, Neal R. and Hamilton, Daniel G. and Karunananthan, Sathya and McDonald, Steve and Moher, David and Nakagawa, Shinichi and Nunan, David and Tugwell, Peter and Welch, Vivian A. and Page, Matthew J.},
	month = oct,
	year = {2022},
	note = {Pages: 2022.04.11.22273688},
	file = {Full Text PDF:C\:\\Users\\neeeb\\Zotero\\storage\\TDVPHLNP\\Nguyen et al. - 2022 - Changing patterns in reporting and sharing of revi.pdf:application/pdf;Snapshot:C\:\\Users\\neeeb\\Zotero\\storage\\U4FZ9CRD\\2022.04.11.html:text/html},
}


@article{nosek2022,
  title = {Replicability, {{Robustness}}, and {{Reproducibility}} in {{Psychological Science}}},
  author = {Nosek, Brian A. and Hardwicke, Tom E. and Moshontz, Hannah and Allard, Aur{\'e}lien and Corker, Katherine S. and Dreber, Anna and Fidler, Fiona and Hilgard, Joe and Kline Struhl, Melissa and Nuijten, Mich{\`e}le B. and Rohrer, Julia M. and Romero, Felipe and Scheel, Anne M. and Scherer, Laura D. and Sch{\"o}nbrodt, Felix D. and Vazire, Simine},
  year = {2022},
  journal = {Annual Review of Psychology},
  volume = {73},
  number = {1},
  pages = {719--748},
  doi = {10.1146/annurev-psych-020821-114157},
  abstract = {Replication\textemdash an important, uncommon, and misunderstood practice\textemdash is gaining appreciation in psychology. Achieving replicability is important for making research progress. If findings are not replicable, then prediction and theory development are stifled. If findings are replicable, then interrogation of their meaning and validity can advance knowledge. Assessing replicability can be productive for generating and testing hypotheses by actively confronting current understandings to identify weaknesses and spur innovation. For psychology, the 2010s might be characterized as a decade of active confrontation. Systematic and multi-site replication projects assessed current understandings and observed surprising failures to replicate many published findings. Replication efforts highlighted sociocultural challenges such as disincentives to conduct replications and a tendency to frame replication as a personal attack rather than a healthy scientific practice, and they raised awareness that replication contributes to self-correction. Nevertheless, innovation in doing and understanding replication and its cousins, reproducibility and robustness, has positioned psychology to improve research practices and accelerate progress.},
  pmid = {34665669},
  keywords = {generalizability,metascience,replication,reproducibility,research methods,robustness,statistical inference,theory,validity},
  annotation = {\_eprint: https://doi.org/10.1146/annurev-psych-020821-114157},
  file = {C\:\\Users\\neeeb\\Zotero\\storage\\G7MZV24H\\Nosek et al. - 2022 - Replicability, Robustness, and Reproducibility in .pdf}
}


@article{opensciencecollaboration2015,
  title = {Estimating the Reproducibility of Psychological Science},
  author = {{Open Science Collaboration}},
  year = {2015},
  month = aug,
  journal = {Science},
  volume = {349},
  number = {6251},
  pages = {aac4716},
  publisher = {{American Association for the Advancement of Science}},
  doi = {10.1126/science.aac4716},
  file = {C\:\\Users\\neeeb\\Zotero\\storage\\STKPLP7C\\OPEN SCIENCE COLLABORATION - 2015 - Estimating the reproducibility of psychological sc.pdf}
}

@article{page2018,
	title = {Reproducible research practices are underused in systematic reviews of biomedical interventions},
	volume = {94},
	issn = {1878-5921},
	doi = {10.1016/j.jclinepi.2017.10.017},
	abstract = {OBJECTIVES: To evaluate how often reproducible research practices, which allow others to recreate the findings of studies, given the original data, are used in systematic reviews (SRs) of biomedical research.
STUDY DESIGN AND SETTING: We evaluated a random sample of SRs indexed in MEDLINE during February 2014, which focused on a therapeutic intervention and reported at least one meta-analysis. Data on reproducible research practices in each SR were extracted using a 26-item form by one author, with a 20\% random sample extracted in duplicate. We explored whether the use of reproducible research practices was associated with an SR being a Cochrane review, as well as with the reported use of the Preferred Reporting Items for Systematic Reviews and Meta-Analyses statement.
RESULTS: We evaluated 110 SRs of therapeutic interventions, 78 (71\%) of which were non-Cochrane SRs. Across the SRs, there were 2,139 meta-analytic effects (including subgroup meta-analytic effects and sensitivity analyses), 1,551 (73\%) of which were reported in sufficient detail to recreate them. Systematic reviewers reported the data needed to recreate all meta-analytic effects in 72 (65\%) SRs only. This percentage was higher in Cochrane than in non-Cochrane SRs (30/32 [94\%] vs. 42/78 [54\%]; risk ratio 1.74, 95\% confidence interval 1.39-2.18). Systematic reviewers who reported imputing, algebraically manipulating, or obtaining some data from the study author/sponsor infrequently stated which specific data were handled in this way. Only 33 (30\%) SRs mentioned access to data sets and statistical code used to perform analyses.
CONCLUSION: Reproducible research practices are underused in SRs of biomedical interventions. Adoption of such practices facilitates identification of errors and allows the SR data to be reanalyzed.},
	language = {eng},
	journal = {Journal of Clinical Epidemiology},
	author = {Page, Matthew J. and Altman, Douglas G. and Shamseer, Larissa and McKenzie, Joanne E. and Ahmadzai, Nadera and Wolfe, Dianna and Yazdi, Fatemeh and Catalá-López, Ferrán and Tricco, Andrea C. and Moher, David},
	month = feb,
	year = {2018},
	pmid = {29113936},
	keywords = {Reproducibility, Systematic reviews, Humans, Research Design, Systematic Reviews as Topic, Reproducibility of Results, Methodology, Biomedical Research, Quality, Data sharing, Reporting},
	pages = {8--18},
	file = {Versión enviada:C\:\\Users\\neeeb\\Zotero\\storage\\ANWJQKZH\\Page et al. - 2018 - Reproducible research practices are underused in s.pdf:application/pdf},
}

@article{page2016,
	title = {Epidemiology and {Reporting} {Characteristics} of {Systematic} {Reviews} of {Biomedical} {Research}: {A} {Cross}-{Sectional} {Study}},
	volume = {13},
	issn = {1549-1676},
	shorttitle = {Epidemiology and {Reporting} {Characteristics} of {Systematic} {Reviews} of {Biomedical} {Research}},
	url = {https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.1002028},
	doi = {10.1371/journal.pmed.1002028},
	abstract = {Background Systematic reviews (SRs) can help decision makers interpret the deluge of published biomedical literature. However, a SR may be of limited use if the methods used to conduct the SR are flawed, and reporting of the SR is incomplete. To our knowledge, since 2004 there has been no cross-sectional study of the prevalence, focus, and completeness of reporting of SRs across different specialties. Therefore, the aim of our study was to investigate the epidemiological and reporting characteristics of a more recent cross-section of SRs. Methods and Findings We searched MEDLINE to identify potentially eligible SRs indexed during the month of February 2014. Citations were screened using prespecified eligibility criteria. Epidemiological and reporting characteristics of a random sample of 300 SRs were extracted by one reviewer, with a 10\% sample extracted in duplicate. We compared characteristics of Cochrane versus non-Cochrane reviews, and the 2014 sample of SRs versus a 2004 sample of SRs. We identified 682 SRs, suggesting that more than 8,000 SRs are being indexed in MEDLINE annually, corresponding to a 3-fold increase over the last decade. The majority of SRs addressed a therapeutic question and were conducted by authors based in China, the UK, or the US; they included a median of 15 studies involving 2,072 participants. Meta-analysis was performed in 63\% of SRs, mostly using standard pairwise methods. Study risk of bias/quality assessment was performed in 70\% of SRs but was rarely incorporated into the analysis (16\%). Few SRs (7\%) searched sources of unpublished data, and the risk of publication bias was considered in less than half of SRs. Reporting quality was highly variable; at least a third of SRs did not report use of a SR protocol, eligibility criteria relating to publication status, years of coverage of the search, a full Boolean search logic for at least one database, methods for data extraction, methods for study risk of bias assessment, a primary outcome, an abstract conclusion that incorporated study limitations, or the funding source of the SR. Cochrane SRs, which accounted for 15\% of the sample, had more complete reporting than all other types of SRs. Reporting has generally improved since 2004, but remains suboptimal for many characteristics. Conclusions An increasing number of SRs are being published, and many are poorly conducted and reported. Strategies are needed to help reduce this avoidable waste in research.},
	language = {en},
	number = {5},
	urldate = {2022-11-03},
	journal = {PLOS Medicine},
	author = {Page, Matthew J. and Shamseer, Larissa and Altman, Douglas G. and Tetzlaff, Jennifer and Sampson, Margaret and Tricco, Andrea C. and Catalá-López, Ferrán and Li, Lun and Reid, Emma K. and Sarkis-Onofre, Rafael and Moher, David},
	month = may,
	year = {2016},
	note = {Publisher: Public Library of Science},
	keywords = {Conflicts of interest, Systematic reviews, Metaanalysis, Database searching, Research reporting guidelines, Publication ethics, Database and informatics methods, Medical risk factors},
	pages = {e1002028},
	file = {Full Text PDF:C\:\\Users\\neeeb\\Zotero\\storage\\7CYQ8B5E\\Page et al. - 2016 - Epidemiology and Reporting Characteristics of Syst.pdf:application/pdf},
}


@article{page2022,
  title = {Data and Code Availability Statements in Systematic Reviews of Interventions Were Often Missing or Inaccurate: A Content Analysis},
  shorttitle = {Data and Code Availability Statements in Systematic Reviews of Interventions Were Often Missing or Inaccurate},
  author = {Page, Matthew J. and Nguyen, Phi-Yen and Hamilton, Daniel G. and Haddaway, Neal R. and Kanukula, Raju and Moher, David and McKenzie, Joanne E.},
  year = {2022},
  month = jul,
  journal = {Journal of Clinical Epidemiology},
  volume = {147},
  pages = {1--10},
  issn = {0895-4356},
  doi = {10.1016/j.jclinepi.2022.03.003},
  abstract = {Objectives To estimate the frequency of data and code availability statements in a random sample of systematic reviews with meta-analysis of aggregate data, summarize the content of the statements and investigate how often data and code files were shared. Methods We searched for systematic reviews with meta-analysis of aggregate data on the effects of a health, social, behavioral, or educational intervention that were indexed in PubMed, Education Collection via ProQuest, Scopus via Elsevier, or Social Sciences Citation Index and Science Citation Index Expanded via Web of Science during a 4-week period (between November 2, and December 2, 2020). Records were randomly sorted and screened independently by two authors until our target sample of 300 systematic reviews was reached. Two authors independently recorded whether a data or code availability statement (or both) appeared in each review and coded the content of the statements using an inductive approach. Results Of the 300 included systematic reviews with meta-analysis, 86 (29\%) had a data availability statement, and seven (2\%) had both a data and code availability statement. In 12/93 (13\%) data availability statements, authors stated that data files were available for download from the journal website or a data repository, which we verified as being true. While 39/93 (42\%) authors stated data were available upon request, 37/93 (40\%) implied that sharing of data files was not necessary or applicable to them, most often because ``all data appear in the article'' or ``no datasets were generated or analyzed''. Discussion Data and code availability statements appear infrequently in systematic review manuscripts. Authors who do provide a data availability statement often incorrectly imply that data sharing is not applicable to systematic reviews. Our results suggest the need for various interventions to increase data and code sharing by systematic reviewers.},
  langid = {english},
  keywords = {Evidence synthesis,Open data,Open science,Open synthesis,Reproducibility of research,Research integrity},
  file = {C\:\\Users\\neeeb\\Zotero\\storage\\WZVMYMEI\\Page et al. - 2022 - Data and code availability statements in systemati.pdf;C\:\\Users\\neeeb\\Zotero\\storage\\AXFW86DS\\S0895435622000646.html}
}


@article{polanin2020,
  title = {Transparency and {{Reproducibility}} of {{Meta-Analyses}} in {{Psychology}}: {{A Meta-Review}}},
  shorttitle = {Transparency and {{Reproducibility}} of {{Meta-Analyses}} in {{Psychology}}},
  author = {Polanin, Joshua R. and Hennessy, Emily A. and Tsuji, Sho},
  year = {2020},
  month = jul,
  journal = {Perspectives on Psychological Science},
  volume = {15},
  number = {4},
  pages = {1026--1041},
  publisher = {{SAGE Publications Inc}},
  issn = {1745-6916},
  doi = {10.1177/1745691620906416},
  abstract = {Systematic review and meta-analysis are possible as viable research techniques only through transparent reporting of primary research; thus, one might expect meta-analysts to demonstrate best practice in their reporting of results and have a high degree of transparency leading to reproducibility of their work. This assumption has yet to be fully tested in the psychological sciences. We therefore aimed to assess the transparency and reproducibility of psychological meta-analyses. We conducted a meta-review by sampling 150 studies from Psychological Bulletin to extract information about each review's transparent and reproducible reporting practices. The results revealed that authors reported on average 55\% of criteria and that transparent reporting practices increased over the three decades studied (b = 1.09, SE = 0.24, t = 4.519, p {$<$} .001). Review authors consistently reported eligibility criteria, effect-size information, and synthesis techniques. Review authors, however, on average, did not report specific search results, screening and extraction procedures, and most importantly, effect-size and moderator information from each individual study. Far fewer studies provided statistical code required for complete analytical replication. We argue that the field of psychology and research synthesis in general should require review authors to report these elements in a transparent and reproducible manner.},
  langid = {english},
  keywords = {meta-analysis,reproducibility,research synthesis,research transparency}
}

  @Manual{r2022,
    title = {R: A Language and Environment for Statistical Computing},
    author = {{R Core Team}},
    organization = {R Foundation for Statistical Computing},
    address = {Vienna, Austria},
    year = {2022},
    url = {https://www.R-project.org/},
  }


@article{tedersoo2021,
  title = {Data Sharing Practices and Data Availability upon Request Differ across Scientific Disciplines},
  author = {Tedersoo, Leho and K{\"u}ngas, Rainer and Oras, Ester and K{\"o}ster, Kajar and Eenmaa, Helen and Leijen, {\"A}li and Pedaste, Margus and Raju, Marju and Astapova, Anastasiya and Lukner, Heli and Kogermann, Karin and Sepp, Tuul},
  year = {2021},
  month = jul,
  journal = {Scientific Data},
  volume = {8},
  number = {1},
  pages = {192},
  publisher = {{Nature Publishing Group}},
  issn = {2052-4463},
  doi = {10.1038/s41597-021-00981-0},
  abstract = {Data sharing is one of the cornerstones of modern science that enables large-scale analyses and reproducibility. We evaluated data availability in research articles across nine disciplines in Nature and Science magazines and recorded corresponding authors' concerns, requests and reasons for declining data sharing. Although data sharing has improved in the last decade and particularly in recent years, data availability and willingness to share data still differ greatly among disciplines. We observed that statements of data availability upon (reasonable) request are inefficient and should not be allowed by journals. To improve data sharing at the time of manuscript acceptance, researchers should be better motivated to release their data with real benefits such as recognition, or bonus points in grant and job applications. We recommend that data management costs should be covered by funding agencies; publicly available research data ought to be included in the evaluation of applications; and surveillance of data sharing should be enforced by both academic publishers and funders. These cross-discipline survey data are available from the plutoF repository.},
  copyright = {2021 The Author(s)},
  langid = {english},
  keywords = {Genetic databases,Molecular ecology},
  annotation = {Bandiera\_abtest: a Cc\_license\_type: cc\_by Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Genetic databases;Molecular ecology Subject\_term\_id: genetic-databases;molecular-ecology},
  file = {C\:\\Users\\neeeb\\Zotero\\storage\\J3ECTZ5V\\Tedersoo et al. - 2021 - Data sharing practices and data availability upon .pdf;C\:\\Users\\neeeb\\Zotero\\storage\\9HECQVG3\\s41597-021-00981-0.html}
}

@article{tendal2009,
  title = {Disagreements in Meta-Analyses Using Outcomes Measured on Continuous or Rating Scales: Observer Agreement Study},
  shorttitle = {Disagreements in Meta-Analyses Using Outcomes Measured on Continuous or Rating Scales},
  author = {Tendal, Britta and Higgins, Julian P. T. and J{\"u}ni, Peter and Hr{\'o}bjartsson, Asbj{\o}rn and Trelle, Sven and N{\"u}esch, Eveline and Wandel, Simon and J{\o}rgensen, Anders W. and Gesser, Katarina and {Ils{\o}e-Kristensen}, S{\o}ren and G{\o}tzsche, Peter C.},
  year = {2009},
  month = aug,
  journal = {BMJ},
  volume = {339},
  pages = {b3128},
  publisher = {{British Medical Journal Publishing Group}},
  issn = {0959-8138, 1468-5833},
  doi = {10.1136/bmj.b3128},
  abstract = {Objective To study the inter-observer variation related to extraction of continuous and numerical rating scale data from trial reports for use in meta-analyses. Design Observer agreement study. Data sources A random sample of 10 Cochrane reviews that presented a result as a standardised mean difference (SMD), the protocols for the reviews and the trial reports (n=45) were retrieved. Data extraction Five experienced methodologists and five PhD students independently extracted data from the trial reports for calculation of the first SMD result in each review. The observers did not have access to the reviews but to the protocols, where the relevant outcome was highlighted. The agreement was analysed at both trial and meta-analysis level, pairing the observers in all possible ways (45 pairs, yielding 2025 pairs of trials and 450 pairs of meta-analyses). Agreement was defined as SMDs that differed less than 0.1 in their point estimates or confidence intervals. Results The agreement was 53\% at trial level and 31\% at meta-analysis level. Including all pairs, the median disagreement was SMD=0.22 (interquartile range 0.07-0.61). The experts agreed somewhat more than the PhD students at trial level (61\% v 46\%), but not at meta-analysis level. Important reasons for disagreement were differences in selection of time points, scales, control groups, and type of calculations; whether to include a trial in the meta-analysis; and data extraction errors made by the observers. In 14 out of the 100 SMDs calculated at the meta-analysis level, individual observers reached different conclusions than the originally published review. Conclusions Disagreements were common and often larger than the effect of commonly used treatments. Meta-analyses using SMDs are prone to observer variation and should be interpreted with caution. The reliability of meta-analyses might be improved by having more detailed review protocols, more than one observer, and statistical expertise.},
  chapter = {Research},
  copyright = {\textcopyright{}  . This is an open-access article distributed under the terms of the Creative Commons Attribution Non-commercial License, which permits use, distribution, and reproduction in any medium, provided the original work is properly cited, the use is non commercial and is otherwise in compliance with the license. See: http://creativecommons.org/licenses/by-nc/2.0/  and  http://creativecommons.org/licenses/by-nc/2.0/legalcode.},
  langid = {english},
  pmid = {19679616},
  file = {C\:\\Users\\neeeb\\Zotero\\storage\\2RWC4V24\\Tendal et al. - 2009 - Disagreements in meta-analyses using outcomes meas.pdf}
}

@article{tendal2011,
	title = {Multiplicity of data in trial reports and the reliability of meta-analyses: empirical study},
	volume = {343},
	copyright = {© Tendal et al 2011. This is an open-access article distributed under the terms of the Creative Commons Attribution Non-commercial License, which permits use, distribution, and reproduction in any medium, provided the original work is properly cited, the use is non commercial and is otherwise in compliance with the license. See: http://creativecommons.org/licenses/by-nc/2.0/  and  http://creativecommons.org/licenses/by-nc/2.0/legalcode.},
	issn = {0959-8138, 1468-5833},
	shorttitle = {Multiplicity of data in trial reports and the reliability of meta-analyses},
	url = {https://www.bmj.com/content/343/bmj.d4829},
	doi = {10.1136/bmj.d4829},
	abstract = {Objectives To examine the extent of multiplicity of data in trial reports and to assess the impact of multiplicity on meta-analysis results.
Design Empirical study on a cohort of Cochrane systematic reviews.
Data sources All Cochrane systematic reviews published from issue 3 in 2006 to issue 2 in 2007 that presented a result as a standardised mean difference (SMD). We retrieved trial reports contributing to the first SMD result in each review, and downloaded review protocols. We used these SMDs to identify a specific outcome for each meta-analysis from its protocol.
Review methods Reviews were eligible if SMD results were based on two to ten randomised trials and if protocols described the outcome. We excluded reviews if they only presented results of subgroup analyses. Based on review protocols and index outcomes, two observers independently extracted the data necessary to calculate SMDs from the original trial reports for any intervention group, time point, or outcome measure compatible with the protocol. From the extracted data, we used Monte Carlo simulations to calculate all possible SMDs for every meta-analysis.
Results We identified 19 eligible meta-analyses (including 83 trials). Published review protocols often lacked information about which data to choose. Twenty-four (29\%) trials reported data for multiple intervention groups, 30 (36\%) reported data for multiple time points, and 29 (35\%) reported the index outcome measured on multiple scales. In 18 meta-analyses, we found multiplicity of data in at least one trial report; the median difference between the smallest and largest SMD results within a meta-analysis was 0.40 standard deviation units (range 0.04 to 0.91).
Conclusions Multiplicity of data can affect the findings of systematic reviews and meta-analyses. To reduce the risk of bias, reviews and meta-analyses should comply with prespecified protocols that clearly identify time points, intervention groups, and scales of interest.},
	language = {en},
	urldate = {2022-11-07},
	journal = {BMJ},
	author = {Tendal, Britta and Nüesch, Eveline and Higgins, Julian P. T. and Jüni, Peter and Gøtzsche, Peter C.},
	month = aug,
	year = {2011},
	pmid = {21878462},
	note = {Publisher: British Medical Journal Publishing Group
Section: Research},
	pages = {d4829},
	file = {Full Text PDF:C\:\\Users\\neeeb\\Zotero\\storage\\PK78RHNN\\Tendal et al. - 2011 - Multiplicity of data in trial reports and the reli.pdf:application/pdf;Snapshot:C\:\\Users\\neeeb\\Zotero\\storage\\J7ET69JX\\bmj.html:text/html},
}


@article{viechtbauer2010,
  title = {Conducting {{Meta-Analyses}} in {{R}} with the Metafor {{Package}}},
  author = {Viechtbauer, Wolfgang},
  year = {2010},
  month = aug,
  journal = {Journal of Statistical Software},
  volume = {36},
  pages = {1--48},
  issn = {1548-7660},
  doi = {10.18637/jss.v036.i03},
  abstract = {The metafor package provides functions for conducting meta-analyses in R. The package includes functions for fitting the meta-analytic fixed- and random-effects models and allows for the inclusion of moderators variables (study-level covariates) in these models. Meta-regression analyses with continuous and categorical moderators can be conducted in this way.  Functions for the Mantel-Haenszel and Peto's one-step method for meta-analyses of 2 x 2 table data are also available. Finally, the package provides various plot functions (for example, for forest, funnel, and radial plots) and functions for assessing the model fit, for obtaining case diagnostics, and for tests of publication bias.},
  copyright = {Copyright (c) 2009 Wolfgang Viechtbauer},
  langid = {english},
  file = {C\:\\Users\\neeeb\\Zotero\\storage\\MU3EDU6P\\Viechtbauer - 2010 - Conducting Meta-Analyses in R with the metafor Pac.pdf}
}

@article{wallach2018,
  title = {Reproducible Research Practices, Transparency, and Open Access Data in the Biomedical Literature, 2015\textendash 2017},
  author = {Wallach, Joshua D. and Boyack, Kevin W. and Ioannidis, John P. A.},
  year = {2018},
  month = nov,
  journal = {PLOS Biology},
  volume = {16},
  number = {11},
  pages = {e2006930},
  publisher = {{Public Library of Science}},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.2006930},
  abstract = {Currently, there is a growing interest in ensuring the transparency and reproducibility of the published scientific literature. According to a previous evaluation of 441 biomedical journals articles published in 2000\textendash 2014, the biomedical literature largely lacked transparency in important dimensions. Here, we surveyed a random sample of 149 biomedical articles published between 2015 and 2017 and determined the proportion reporting sources of public and/or private funding and conflicts of interests, sharing protocols and raw data, and undergoing rigorous independent replication and reproducibility checks. We also investigated what can be learned about reproducibility and transparency indicators from open access data provided on PubMed. The majority of the 149 studies disclosed some information regarding funding (103, 69.1\% [95\% confidence interval, 61.0\% to 76.3\%]) or conflicts of interest (97, 65.1\% [56.8\% to 72.6\%]). Among the 104 articles with empirical data in which protocols or data sharing would be pertinent, 19 (18.3\% [11.6\% to 27.3\%]) discussed publicly available data; only one (1.0\% [0.1\% to 6.0\%]) included a link to a full study protocol. Among the 97 articles in which replication in studies with different data would be pertinent, there were five replication efforts (5.2\% [1.9\% to 12.2\%]). Although clinical trial identification numbers and funding details were often provided on PubMed, only two of the articles without a full text article in PubMed Central that discussed publicly available data at the full text level also contained information related to data sharing on PubMed; none had a conflicts of interest statement on PubMed. Our evaluation suggests that although there have been improvements over the last few years in certain key indicators of reproducibility and transparency, opportunities exist to improve reproducible research practices across the biomedical literature and to make features related to reproducibility more readily visible in PubMed.},
  langid = {english},
  keywords = {Conflicts of interest,Government funding of science,Medical journals,Open science,Replication studies,Reproducibility,Scientific publishing,Systematic reviews},
  file = {C\:\\Users\\neeeb\\Zotero\\storage\\9AF7FZE9\\Wallach et al. - 2018 - Reproducible research practices, transparency, and.pdf;C\:\\Users\\neeeb\\Zotero\\storage\\PYXZMJ4G\\article.html}
}

@article{wayant2019,
	title = {Evaluation of {Reproducible} {Research} {Practices} in {Oncology} {Systematic} {Reviews} {With} {Meta}-analyses {Referenced} by {National} {Comprehensive} {Cancer} {Network} {Guidelines}},
	volume = {5},
	issn = {2374-2437},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6735674/},
	doi = {10.1001/jamaoncol.2019.2564},
	abstract = {This cross-sectional study evaluates the reproducibility of research practices in oncology systematic reviews with meta-analyses cited by the National Comprehensive Cancer Network guidelines for the treatment of cancer by site.},
	number = {11},
	urldate = {2022-11-03},
	journal = {JAMA Oncology},
	author = {Wayant, Cole and Page, Matthew J. and Vassar, Matt},
	month = nov,
	year = {2019},
	pmid = {31486837},
	pmcid = {PMC6735674},
	pages = {1550--1555},
	file = {Texto completo:C\:\\Users\\neeeb\\Zotero\\storage\\7EN7VSLI\\Wayant et al. - 2019 - Evaluation of Reproducible Research Practices in O.pdf:application/pdf},
}

@article{wicherts2006,
  title={The poor availability of psychological research data for reanalysis.},
  author={Wicherts, Jelte M and Borsboom, Denny and Kats, Judith and Molenaar, Dylan},
  journal={American psychologist},
  volume={61},
  number={7},
  pages={726},
  year={2006},
  publisher={American Psychological Association},
  doi= {10.1037/0003-066X.61.7.726}
}

@article{wilkinson2016,
  title = {The {{FAIR Guiding Principles}} for Scientific Data Management and Stewardship},
  author = {Wilkinson, Mark D. and Dumontier, Michel and Aalbersberg, IJsbrand Jan and Appleton, Gabrielle and Axton, Myles and Baak, Arie and Blomberg, Niklas and Boiten, Jan-Willem and {da Silva Santos}, Luiz Bonino and Bourne, Philip E. and Bouwman, Jildau and Brookes, Anthony J. and Clark, Tim and Crosas, Merc{\`e} and Dillo, Ingrid and Dumon, Olivier and Edmunds, Scott and Evelo, Chris T. and Finkers, Richard and {Gonzalez-Beltran}, Alejandra and Gray, Alasdair J. G. and Groth, Paul and Goble, Carole and Grethe, Jeffrey S. and Heringa, Jaap and {'t Hoen}, Peter A. C. and Hooft, Rob and Kuhn, Tobias and Kok, Ruben and Kok, Joost and Lusher, Scott J. and Martone, Maryann E. and Mons, Albert and Packer, Abel L. and Persson, Bengt and {Rocca-Serra}, Philippe and Roos, Marco and {van Schaik}, Rene and Sansone, Susanna-Assunta and Schultes, Erik and Sengstag, Thierry and Slater, Ted and Strawn, George and Swertz, Morris A. and Thompson, Mark and {van der Lei}, Johan and {van Mulligen}, Erik and Velterop, Jan and Waagmeester, Andra and Wittenburg, Peter and Wolstencroft, Katherine and Zhao, Jun and Mons, Barend},
  year = {2016},
  month = mar,
  journal = {Scientific Data},
  volume = {3},
  number = {1},
  pages = {160018},
  publisher = {{Nature Publishing Group}},
  issn = {2052-4463},
  doi = {10.1038/sdata.2016.18},
  abstract = {There is an urgent need to improve the infrastructure supporting the reuse of scholarly data. A diverse set of stakeholders\textemdash representing academia, industry, funding agencies, and scholarly publishers\textemdash have come together to design and jointly endorse a concise and measureable set of principles that we refer to as the FAIR Data Principles. The intent is that these may act as a guideline for those wishing to enhance the reusability of their data holdings. Distinct from peer initiatives that focus on the human scholar, the FAIR Principles put specific emphasis on enhancing the ability of machines to automatically find and use the data, in addition to supporting its reuse by individuals. This Comment is the first formal publication of the FAIR Principles, and includes the rationale behind them, and some exemplar implementations in the community.},
  copyright = {2016 The Author(s)},
  langid = {english},
  keywords = {Publication characteristics,Research data},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Comments \& Opinion Subject\_term: Publication characteristics;Research data Subject\_term\_id: publication-characteristics;research-data},
  file = {C\:\\Users\\neeeb\\Zotero\\storage\\8SBMLHAQ\\Wilkinson et al. - 2016 - The FAIR Guiding Principles for scientific data ma.pdf;C\:\\Users\\neeeb\\Zotero\\storage\\4JPG5ETU\\sdata201618.html}
}


@Manual{xie2022,
    title = {knitr: A General-Purpose Package for Dynamic Report
      Generation in R},
    author = {Yihui Xie},
    year = {2022},
    note = {R package version 1.39},
    url = {https://yihui.org/knitr/},
  }

